{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a4fd68",
   "metadata": {},
   "source": [
    "# Geometric Interpretation of Overfitting in Function Space\n",
    "\n",
    "## Objective\n",
    "Understand what happens **geometrically** in function space when a model achieves low training error but high validation error. We'll go beyond the superficial \"it's overfitting\" to visualize:\n",
    "\n",
    "1. **Function space geometry**: How learned functions deviate from the true underlying function\n",
    "2. **Hypothesis space coverage**: How complex models can fit any training configuration\n",
    "3. **Noise fitting**: What it means geometrically to \"memorize noise\"\n",
    "4. **Curvature and complexity**: How model capacity affects the smoothness of decision boundaries\n",
    "\n",
    "## Key Insight\n",
    "High validation error despite low training error means the learned function has high curvature or complexity in regions between training points, creating a **different geometric structure** than the true underlying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe83317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9dd2ad",
   "metadata": {},
   "source": [
    "## Part 1: Generating Synthetic Data\n",
    "\n",
    "We'll create a 1D regression problem where:\n",
    "- **True function**: A simple, smooth function (e.g., sine wave or polynomial)\n",
    "- **Observed data**: True function + Gaussian noise\n",
    "- **Goal**: Recover the true function from noisy observations\n",
    "\n",
    "This setup lets us visualize the **function space** directly and see how learned functions compare to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8687001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=100, noise_level=0.3):\n",
    "    \"\"\"\n",
    "    Generate synthetic 1D regression data\n",
    "    True function: f(x) = sin(2πx) + 0.5*sin(6πx)\n",
    "    \"\"\"\n",
    "    # Training data\n",
    "    X_train = np.sort(np.random.uniform(0, 1, n_samples))\n",
    "    \n",
    "    # True underlying function (smooth, low-frequency components)\n",
    "    y_true_train = np.sin(2 * np.pi * X_train) + 0.5 * np.sin(6 * np.pi * X_train)\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, noise_level, n_samples)\n",
    "    y_train = y_true_train + noise\n",
    "    \n",
    "    # Dense grid for visualization (true function)\n",
    "    X_grid = np.linspace(0, 1, 500)\n",
    "    y_grid_true = np.sin(2 * np.pi * X_grid) + 0.5 * np.sin(6 * np.pi * X_grid)\n",
    "    \n",
    "    return X_train, y_train, y_true_train, X_grid, y_grid_true\n",
    "\n",
    "# Generate data\n",
    "X_train, y_train, y_true_train, X_grid, y_grid_true = generate_data(n_samples=50)\n",
    "\n",
    "# Create validation set (different samples from same distribution)\n",
    "X_val, y_val, y_true_val, _, _ = generate_data(n_samples=30)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c52621",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "# Plot true function\n",
    "ax.plot(X_grid, y_grid_true, 'g-', linewidth=2, label='True Function', alpha=0.8)\n",
    "\n",
    "# Plot training data\n",
    "ax.scatter(X_train, y_train, c='blue', s=50, alpha=0.6, \n",
    "           edgecolors='black', label='Training Data (with noise)')\n",
    "\n",
    "# Plot validation data\n",
    "ax.scatter(X_val, y_val, c='red', s=50, alpha=0.6, \n",
    "           marker='s', edgecolors='black', label='Validation Data')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Synthetic Dataset: True Function vs Noisy Observations', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc52903",
   "metadata": {},
   "source": [
    "## Part 2: Fitting Models of Varying Complexity\n",
    "\n",
    "We'll fit polynomial models of different degrees:\n",
    "- **Low complexity (degree 3)**: Underfitting - too simple to capture the pattern\n",
    "- **Moderate complexity (degree 9)**: Good fit - captures signal without overfitting\n",
    "- **High complexity (degree 20)**: Overfitting - memorizes training noise\n",
    "\n",
    "### Geometric Interpretation\n",
    "In function space, each polynomial defines a curve. High-degree polynomials can create **highly oscillatory functions** that pass through all training points but diverge wildly between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial(X_train, y_train, X_test, degree):\n",
    "    \"\"\"Fit polynomial regression of given degree\"\"\"\n",
    "    # Reshape for sklearn\n",
    "    X_train_reshaped = X_train.reshape(-1, 1)\n",
    "    X_test_reshaped = X_test.reshape(-1, 1)\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train_reshaped)\n",
    "    X_test_poly = poly.transform(X_test_reshaped)\n",
    "    \n",
    "    # Fit linear regression on polynomial features\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    return y_test_pred, y_train_pred, model, poly\n",
    "\n",
    "# Train models with different complexities\n",
    "degrees = [3, 9, 20]\n",
    "models_results = {}\n",
    "\n",
    "for degree in degrees:\n",
    "    y_grid_pred, y_train_pred, model, poly = fit_polynomial(\n",
    "        X_train, y_train, X_grid, degree\n",
    "    )\n",
    "    # CORRECTED: y_val_pred should be the FIRST return value\n",
    "    y_val_pred, _, _, _ = fit_polynomial(\n",
    "        X_train, y_train, X_val, degree\n",
    "    )\n",
    "    \n",
    "    train_error = mean_squared_error(y_train, y_train_pred)\n",
    "    val_error = mean_squared_error(y_val, y_val_pred)\n",
    "    \n",
    "    models_results[degree] = {\n",
    "        'predictions': y_grid_pred,\n",
    "        'train_error': train_error,\n",
    "        'val_error': val_error,\n",
    "        'model': model,\n",
    "        'poly': poly\n",
    "    }\n",
    "    \n",
    "    print(f\"Degree {degree:2d} | Train MSE: {train_error:.4f} | Val MSE: {val_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce3b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, degree in enumerate(degrees):\n",
    "    ax = axes[idx]\n",
    "    results = models_results[degree]\n",
    "    \n",
    "    # True function\n",
    "    ax.plot(X_grid, y_grid_true, 'g-', linewidth=2, \n",
    "            label='True Function', alpha=0.7)\n",
    "    \n",
    "    # Learned function\n",
    "    ax.plot(X_grid, results['predictions'], 'r-', linewidth=2.5, \n",
    "            label=f'Learned (degree {degree})', alpha=0.8)\n",
    "    \n",
    "    # Training points\n",
    "    ax.scatter(X_train, y_train, c='blue', s=40, alpha=0.6, \n",
    "               edgecolors='black', zorder=5, label='Training Data')\n",
    "    \n",
    "    # Title with errors\n",
    "    ax.set_title(f'Degree {degree}\\nTrain MSE: {results[\"train_error\"]:.4f} | '\n",
    "                 f'Val MSE: {results[\"val_error\"]:.4f}', fontsize=11)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([-3, 3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a1b3e",
   "metadata": {},
   "source": [
    "## Part 3: Geometric Analysis in Function Space\n",
    "\n",
    "### What We Observe:\n",
    "\n",
    "1. **Degree 3 (Underfitting)**: \n",
    "   - Cannot capture the complexity of the true function\n",
    "   - Both train and validation errors are high\n",
    "   - Geometrically: The hypothesis space is too constrained\n",
    "\n",
    "2. **Degree 9 (Good Fit)**:\n",
    "   - Captures the underlying pattern\n",
    "   - Low errors on both sets\n",
    "   - Geometrically: Close to the true function in most regions\n",
    "\n",
    "3. **Degree 20 (Overfitting)**:\n",
    "   - **Low training error** (passes near all training points)\n",
    "   - **High validation error** (wild oscillations between points)\n",
    "   - Geometrically: High curvature, fits noise instead of signal\n",
    "\n",
    "### Key Geometric Insight:\n",
    "The degree-20 polynomial creates **high-frequency oscillations** in regions without training data. In function space, it's far from the true function despite passing through the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pointwise error from true function\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, degree in enumerate(degrees):\n",
    "    ax = axes[idx]\n",
    "    results = models_results[degree]\n",
    "    \n",
    "    # Calculate deviation from true function\n",
    "    deviation = np.abs(results['predictions'] - y_grid_true)\n",
    "    \n",
    "    ax.fill_between(X_grid, 0, deviation, alpha=0.4, color='red', \n",
    "                     label='|Learned - True|')\n",
    "    ax.plot(X_grid, deviation, 'r-', linewidth=2)\n",
    "    \n",
    "    # Mark training points\n",
    "    ax.scatter(X_train, np.zeros_like(X_train), c='blue', s=40, \n",
    "               marker='|', linewidths=3, label='Training Points')\n",
    "    \n",
    "    ax.set_title(f'Function Space Distance (Degree {degree})\\n'\n",
    "                 f'Mean Deviation: {np.mean(deviation):.4f}', fontsize=11)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('|f_learned(x) - f_true(x)|')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"High-degree polynomial has LARGE deviations between training points,\")\n",
    "print(\"showing it has learned a geometrically different function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24dc79",
   "metadata": {},
   "source": [
    "## Part 4: Curvature and Complexity\n",
    "\n",
    "**Curvature** measures how quickly a function changes direction. High curvature = rapid oscillations.\n",
    "\n",
    "Mathematically, curvature is related to the second derivative. Let's visualize:\n",
    "- The learned function (f(x))\n",
    "- Its first derivative (f'(x)) - rate of change\n",
    "- Its second derivative (f''(x)) - curvature\n",
    "\n",
    "High curvature between training points is the geometric signature of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(x, y, order=1):\n",
    "    \"\"\"Compute numerical derivatives\"\"\"\n",
    "    if order == 1:\n",
    "        return np.gradient(y, x)\n",
    "    elif order == 2:\n",
    "        dy = np.gradient(y, x)\n",
    "        return np.gradient(dy, x)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "for col_idx, degree in enumerate(degrees):\n",
    "    results = models_results[degree]\n",
    "    y_pred = results['predictions']\n",
    "    \n",
    "    # Row 0: Function itself\n",
    "    axes[0, col_idx].plot(X_grid, y_grid_true, 'g-', linewidth=2, \n",
    "                          label='True', alpha=0.7)\n",
    "    axes[0, col_idx].plot(X_grid, y_pred, 'r-', linewidth=2, \n",
    "                          label='Learned', alpha=0.8)\n",
    "    axes[0, col_idx].scatter(X_train, y_train, c='blue', s=30, alpha=0.5)\n",
    "    axes[0, col_idx].set_title(f'Degree {degree}: Function', fontsize=11)\n",
    "    axes[0, col_idx].set_ylabel('f(x)')\n",
    "    axes[0, col_idx].legend(fontsize=9)\n",
    "    axes[0, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 1: First derivative\n",
    "    dy_true = numerical_derivative(X_grid, y_grid_true, order=1)\n",
    "    dy_pred = numerical_derivative(X_grid, y_pred, order=1)\n",
    "    \n",
    "    axes[1, col_idx].plot(X_grid, dy_true, 'g-', linewidth=2, alpha=0.7)\n",
    "    axes[1, col_idx].plot(X_grid, dy_pred, 'r-', linewidth=2, alpha=0.8)\n",
    "    axes[1, col_idx].set_title(f\"First Derivative f'(x)\", fontsize=11)\n",
    "    axes[1, col_idx].set_ylabel(\"f'(x)\")\n",
    "    axes[1, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 2: Second derivative (curvature)\n",
    "    d2y_true = numerical_derivative(X_grid, y_grid_true, order=2)\n",
    "    d2y_pred = numerical_derivative(X_grid, y_pred, order=2)\n",
    "    \n",
    "    axes[2, col_idx].plot(X_grid, d2y_true, 'g-', linewidth=2, alpha=0.7)\n",
    "    axes[2, col_idx].plot(X_grid, d2y_pred, 'r-', linewidth=2, alpha=0.8)\n",
    "    axes[2, col_idx].set_title(f'Second Derivative f\"(x) (Curvature)', fontsize=11)\n",
    "    axes[2, col_idx].set_ylabel('f\"(x)')\n",
    "    axes[2, col_idx].set_xlabel('x')\n",
    "    axes[2, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate curvature metrics\n",
    "    curvature_true = np.mean(np.abs(d2y_true))\n",
    "    curvature_pred = np.mean(np.abs(d2y_pred))\n",
    "    axes[2, col_idx].text(0.5, 0.95, \n",
    "                          f'Avg |curvature|:\\nTrue: {curvature_true:.2f}\\nLearned: {curvature_pred:.2f}',\n",
    "                          transform=axes[2, col_idx].transAxes,\n",
    "                          verticalalignment='top',\n",
    "                          bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "                          fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e56fa",
   "metadata": {},
   "source": [
    "## Part 5: Hypothesis Space Geometry (2D Visualization)\n",
    "\n",
    "Let's visualize the **hypothesis space** geometrically. Each function can be represented as a point in a high-dimensional space where:\n",
    "- Each dimension corresponds to a function value at a specific location\n",
    "- Training constrains the function to pass near certain points\n",
    "- The model searches for a function in this space\n",
    "\n",
    "We'll project this into 2D using function values at two different x locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc6282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose two x locations for 2D projection\n",
    "x1, x2 = 0.25, 0.75\n",
    "idx1 = np.argmin(np.abs(X_grid - x1))\n",
    "idx2 = np.argmin(np.abs(X_grid - x2))\n",
    "\n",
    "# True function values at these points\n",
    "true_f1 = y_grid_true[idx1]\n",
    "true_f2 = y_grid_true[idx2]\n",
    "\n",
    "# Collect learned function values\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "colors = {3: 'blue', 9: 'green', 20: 'red'}\n",
    "markers = {3: 'o', 9: 's', 20: '^'}\n",
    "\n",
    "for degree in degrees:\n",
    "    results = models_results[degree]\n",
    "    f1 = results['predictions'][idx1]\n",
    "    f2 = results['predictions'][idx2]\n",
    "    \n",
    "    ax.scatter(f1, f2, s=200, c=colors[degree], marker=markers[degree],\n",
    "               alpha=0.7, edgecolors='black', linewidths=2,\n",
    "               label=f'Degree {degree} (Train MSE: {results[\"train_error\"]:.3f})')\n",
    "\n",
    "# True function point\n",
    "ax.scatter(true_f1, true_f2, s=300, c='gold', marker='*',\n",
    "           edgecolors='black', linewidths=2, label='True Function', zorder=10)\n",
    "\n",
    "ax.set_xlabel(f'f(x={x1})', fontsize=14)\n",
    "ax.set_ylabel(f'f(x={x2})', fontsize=14)\n",
    "ax.set_title('Hypothesis Space: 2D Projection\\n(Each point = entire function)', \n",
    "             fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(true_f2, color='gold', linestyle='--', alpha=0.3)\n",
    "ax.axvline(true_f1, color='gold', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add distance annotations\n",
    "for degree in degrees:\n",
    "    results = models_results[degree]\n",
    "    f1 = results['predictions'][idx1]\n",
    "    f2 = results['predictions'][idx2]\n",
    "    distance = np.sqrt((f1 - true_f1)**2 + (f2 - true_f2)**2)\n",
    "    ax.annotate(f'd={distance:.3f}', \n",
    "                xy=(f1, f2), xytext=(5, 5),\n",
    "                textcoords='offset points', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Each point represents an entire function\")\n",
    "print(\"- Distance from gold star = how different the learned function is from truth\")\n",
    "print(\"- Degree-20 may have low training error but is geometrically far from truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307b30d",
   "metadata": {},
   "source": [
    "## Part 6: Fitting Signal vs. Fitting Noise\n",
    "\n",
    "Let's decompose what each model learns:\n",
    "- **Signal**: The true underlying pattern (smooth function)\n",
    "- **Noise**: Random perturbations in training data\n",
    "\n",
    "We'll visualize how much of each component the models fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645494d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for col_idx, degree in enumerate(degrees):\n",
    "    results = models_results[degree]\n",
    "    y_pred = results['predictions']\n",
    "    \n",
    "    # Get predictions at training points\n",
    "    _, y_train_pred, _, _ = fit_polynomial(X_train, y_train, X_train, degree)\n",
    "    \n",
    "    # Decompose training predictions\n",
    "    signal_component = y_true_train  # True function values at training points\n",
    "    noise_component = y_train - y_true_train  # Noise in training data\n",
    "    \n",
    "    fitted_signal = y_train_pred - noise_component  # Approx how much signal was fit\n",
    "    fitted_noise = y_train_pred - signal_component  # Approx how much noise was fit\n",
    "    \n",
    "    # Top row: Signal fitting\n",
    "    axes[0, col_idx].scatter(X_train, signal_component, c='green', s=60, \n",
    "                             alpha=0.6, label='True Signal', marker='o')\n",
    "    axes[0, col_idx].scatter(X_train, fitted_signal, c='red', s=60, \n",
    "                             alpha=0.6, label='Fitted Signal', marker='x')\n",
    "    axes[0, col_idx].plot([min(X_train), max(X_train)], \n",
    "                          [min(X_train), max(X_train)], \n",
    "                          'k--', alpha=0.3)\n",
    "    axes[0, col_idx].set_title(f'Degree {degree}: Signal Fitting', fontsize=11)\n",
    "    axes[0, col_idx].set_xlabel('Training Point Index')\n",
    "    axes[0, col_idx].set_ylabel('Signal Value')\n",
    "    axes[0, col_idx].legend(fontsize=9)\n",
    "    axes[0, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bottom row: Noise fitting\n",
    "    axes[1, col_idx].scatter(X_train, noise_component, c='orange', s=60, \n",
    "                             alpha=0.6, label='True Noise', marker='o')\n",
    "    axes[1, col_idx].scatter(X_train, fitted_noise, c='purple', s=60, \n",
    "                             alpha=0.6, label='Fitted Noise', marker='x')\n",
    "    axes[1, col_idx].axhline(0, color='black', linestyle='--', alpha=0.3)\n",
    "    axes[1, col_idx].set_title(f'Degree {degree}: Noise Fitting', fontsize=11)\n",
    "    axes[1, col_idx].set_xlabel('Training Point Index')\n",
    "    axes[1, col_idx].set_ylabel('Noise Value')\n",
    "    axes[1, col_idx].legend(fontsize=9)\n",
    "    axes[1, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    signal_corr = np.corrcoef(signal_component, fitted_signal)[0, 1]\n",
    "    noise_corr = np.corrcoef(noise_component, fitted_noise)[0, 1]\n",
    "    \n",
    "    axes[0, col_idx].text(0.05, 0.95, f'Corr: {signal_corr:.3f}',\n",
    "                          transform=axes[0, col_idx].transAxes,\n",
    "                          verticalalignment='top',\n",
    "                          bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
    "                          fontsize=10)\n",
    "    \n",
    "    axes[1, col_idx].text(0.05, 0.95, f'Corr: {noise_corr:.3f}',\n",
    "                          transform=axes[1, col_idx].transAxes,\n",
    "                          verticalalignment='top',\n",
    "                          bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7),\n",
    "                          fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"High-degree polynomials show high correlation with NOISE,\")\n",
    "print(\"meaning they've learned the random perturbations, not just the pattern.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0776457",
   "metadata": {},
   "source": [
    "## Summary: Geometric Interpretation of Low Train Error + High Val Error\n",
    "\n",
    "### What We've Shown:\n",
    "\n",
    "1. **Function Space Distance**: \n",
    "   - Overfit models create functions that are geometrically **far** from the true function\n",
    "   - Despite low training error, they diverge significantly between training points\n",
    "\n",
    "2. **High Curvature**:\n",
    "   - Overfit models have **excessive curvature** (high second derivatives)\n",
    "   - This creates oscillations that fit training noise but fail to generalize\n",
    "\n",
    "3. **Hypothesis Space**:\n",
    "   - In the space of all possible functions, overfit models lie far from the true function\n",
    "   - Training data alone doesn't constrain them to the right region\n",
    "\n",
    "4. **Noise vs Signal**:\n",
    "   - Complex models can fit both signal AND noise\n",
    "   - Low training error means fitting EVERYTHING, including noise\n",
    "   - High validation error reveals that noise patterns don't repeat\n",
    "\n",
    "### Key Geometric Insight:\n",
    "\n",
    "**\"Overfitting\" geometrically means**: The learned function f̂(x) has high-frequency components or high curvature that make it:\n",
    "- Pass through (or near) all training points → low training error\n",
    "- Deviate significantly from the true function f*(x) in interpolation regions → high validation error\n",
    "\n",
    "The model has learned a **different function** from a different part of hypothesis space, not just a \"noisy version\" of the right function.\n",
    "\n",
    "### Mathematical Formulation:\n",
    "```\n",
    "Training Error ≈ 0  ⟹  f̂(x_train) ≈ y_train\n",
    "Validation Error ≫ 0  ⟹  ||f̂(x) - f*(x)||_L2 is large\n",
    "\n",
    "This happens when f̂ has high complexity/curvature:\n",
    "∫ |f̂''(x)|² dx ≫ ∫ |f*''(x)|² dx\n",
    "```\n",
    "\n",
    "### Remedies:\n",
    "- Regularization (penalty on curvature/complexity)\n",
    "- More training data (constrains function in more regions)\n",
    "- Simpler models (smaller hypothesis space)\n",
    "- Ensemble methods (average out high-frequency noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary visualization\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Top left: Training and validation errors\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "train_errors = [models_results[d]['train_error'] for d in degrees]\n",
    "val_errors = [models_results[d]['val_error'] for d in degrees]\n",
    "\n",
    "x_pos = np.arange(len(degrees))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x_pos - width/2, train_errors, width, label='Train Error', \n",
    "                color='blue', alpha=0.7)\n",
    "bars2 = ax1.bar(x_pos + width/2, val_errors, width, label='Val Error', \n",
    "                color='red', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Polynomial Degree')\n",
    "ax1.set_ylabel('Mean Squared Error')\n",
    "ax1.set_title('Training vs Validation Error')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(degrees)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight overfitting regime\n",
    "ax1.axvspan(1.5, 2.5, alpha=0.2, color='red', label='Overfitting')\n",
    "\n",
    "# Top right: Generalization gap\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "gen_gaps = [val_errors[i] - train_errors[i] for i in range(len(degrees))]\n",
    "\n",
    "bars = ax2.bar(degrees, gen_gaps, color=['green', 'yellow', 'red'], \n",
    "               alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax2.set_xlabel('Polynomial Degree')\n",
    "ax2.set_ylabel('Generalization Gap')\n",
    "ax2.set_title('Validation Error - Training Error\\n(Overfitting Indicator)')\n",
    "ax2.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (degree, gap) in enumerate(zip(degrees, gen_gaps)):\n",
    "    ax2.text(degree, gap + 0.01, f'{gap:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# Middle: All learned functions\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "ax3.plot(X_grid, y_grid_true, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "\n",
    "colors_funcs = {3: 'blue', 9: 'green', 20: 'red'}\n",
    "for degree in degrees:\n",
    "    results = models_results[degree]\n",
    "    ax3.plot(X_grid, results['predictions'], \n",
    "             color=colors_funcs[degree], linewidth=2, \n",
    "             label=f'Degree {degree}', alpha=0.7)\n",
    "\n",
    "ax3.scatter(X_train, y_train, c='gray', s=40, alpha=0.5, \n",
    "            edgecolors='black', zorder=5, label='Training Data')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_title('All Learned Functions vs True Function', fontsize=13)\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([-3, 3])\n",
    "\n",
    "# Bottom: Distance from true function\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "for degree in degrees:\n",
    "    results = models_results[degree]\n",
    "    deviation = np.abs(results['predictions'] - y_grid_true)\n",
    "    ax4.plot(X_grid, deviation, linewidth=2, \n",
    "             label=f'Degree {degree} | Avg: {np.mean(deviation):.3f}',\n",
    "             color=colors_funcs[degree])\n",
    "\n",
    "# Shade regions without training data\n",
    "for i in range(len(X_train)-1):\n",
    "    if X_train[i+1] - X_train[i] > 0.05:  # Gap in training data\n",
    "        ax4.axvspan(X_train[i], X_train[i+1], alpha=0.1, color='red')\n",
    "\n",
    "ax4.scatter(X_train, np.zeros_like(X_train), c='black', s=50, \n",
    "            marker='|', linewidths=3, label='Training Points', zorder=10)\n",
    "ax4.set_xlabel('x')\n",
    "ax4.set_ylabel('|f_learned(x) - f_true(x)|')\n",
    "ax4.set_title('Geometric Distance from True Function\\n(Red regions = gaps in training data)', \n",
    "              fontsize=13)\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Complete Geometric Analysis: Overfitting in Function Space', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Degree 20 achieves training MSE of {models_results[20]['train_error']:.4f}\")\n",
    "print(f\"But validation MSE is {models_results[20]['val_error']:.4f}\")\n",
    "print(f\"\\nGeometrically, this is because the learned function has:\")\n",
    "print(f\"  - Mean deviation from truth: {np.mean(np.abs(models_results[20]['predictions'] - y_grid_true)):.3f}\")\n",
    "print(f\"  - High curvature between training points\")\n",
    "print(f\"  - Fits noise patterns that don't generalize\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d1e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
