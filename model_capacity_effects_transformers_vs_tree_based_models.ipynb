{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6405eb9d",
   "metadata": {},
   "source": [
    "# Model Capacity Effects: Transformers vs Tree-Based Models\n",
    "\n",
    "## Objective\n",
    "Understand how **increasing model capacity** affects bias and variance differently in:\n",
    "1. **Tree-based models** (Decision Trees, Random Forests, Gradient Boosting)\n",
    "2. **Transformer-based models** (Attention mechanisms, deep architectures)\n",
    "\n",
    "## Key Question\n",
    "Why do these architectures respond differently to capacity increases?\n",
    "\n",
    "## Hypothesis\n",
    "- **Trees**: Sharp transition from underfitting to overfitting as capacity increases (depth/trees)\n",
    "- **Transformers**: More gradual scaling, can benefit from overparameterization due to implicit regularization\n",
    "\n",
    "## What We'll Explore\n",
    "1. Generate synthetic data with complex feature interactions\n",
    "2. Train models with varying capacity levels\n",
    "3. Measure bias, variance, and generalization at each capacity\n",
    "4. Visualize decision boundaries\n",
    "5. Explain the fundamental architectural differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a6f6e",
   "metadata": {},
   "source": [
    "## Part 1: Synthetic Dataset Design\n",
    "\n",
    "We need a dataset that:\n",
    "1. Has **complex feature interactions** (benefits transformers' attention)\n",
    "2. Has **non-linear patterns** (challenges simple trees)\n",
    "3. Is **high-dimensional** (shows capacity scaling effects)\n",
    "\n",
    "### Dataset Characteristics:\n",
    "- **Task**: Multi-class classification (3 classes)\n",
    "- **Features**: 20 features with various interaction patterns\n",
    "- **Interactions**: \n",
    "  - Multiplicative interactions (feature_i × feature_j)\n",
    "  - Non-linear transformations\n",
    "  - Long-range dependencies (simulates sequential context)\n",
    "- **Sample size**: Moderate (to see overfitting effects)\n",
    "\n",
    "This mimics scenarios where:\n",
    "- Trees need depth to capture interactions\n",
    "- Transformers can use attention to capture relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complex_dataset(n_samples=1000, n_features=20, noise_level=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic dataset with complex feature interactions.\n",
    "    Designed to test both tree-based and attention-based models.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate base features\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Create complex decision boundaries using multiple interaction patterns\n",
    "    \n",
    "    # Pattern 1: Multiplicative interactions (features 0-5)\n",
    "    interaction_1 = (X[:, 0] * X[:, 1]) + (X[:, 2] * X[:, 3])\n",
    "    \n",
    "    # Pattern 2: Non-linear transformations (features 6-10)\n",
    "    interaction_2 = np.sin(X[:, 6]) * np.cos(X[:, 7]) + np.tanh(X[:, 8] * X[:, 9])\n",
    "    \n",
    "    # Pattern 3: Long-range dependencies (features 11-19)\n",
    "    # Simulate sequential context: each feature depends on previous ones\n",
    "    interaction_3 = np.sum([X[:, i] * X[:, i+1] * (i/n_features) \n",
    "                           for i in range(11, 18)], axis=0)\n",
    "    \n",
    "    # Pattern 4: XOR-like pattern (features 15-16)\n",
    "    interaction_4 = ((X[:, 15] > 0) ^ (X[:, 16] > 0)).astype(float) * 2 - 1\n",
    "    \n",
    "    # Combine patterns to create target\n",
    "    decision_function = (\n",
    "        0.4 * interaction_1 + \n",
    "        0.3 * interaction_2 + \n",
    "        0.2 * interaction_3 + \n",
    "        0.1 * interaction_4\n",
    "    )\n",
    "    \n",
    "    # Add noise\n",
    "    decision_function += np.random.normal(0, noise_level, n_samples)\n",
    "    \n",
    "    # Create 3 classes based on quantiles\n",
    "    quantiles = np.percentile(decision_function, [33, 67])\n",
    "    y = np.zeros(n_samples, dtype=int)\n",
    "    y[decision_function > quantiles[1]] = 2\n",
    "    y[(decision_function > quantiles[0]) & (decision_function <= quantiles[1])] = 1\n",
    "    \n",
    "    return X, y, decision_function\n",
    "\n",
    "# Generate datasets\n",
    "print(\"Generating datasets...\")\n",
    "X_full, y_full, decision_values = generate_complex_dataset(\n",
    "    n_samples=2000, n_features=20, noise_level=0.15\n",
    ")\n",
    "\n",
    "# Split into train, validation, and test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.3, random_state=42, stratify=y_full\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "print(f\"\\nClass distribution (train): {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce484e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first two principal components and feature correlations\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Data in 2D PCA space\n",
    "scatter = axes[0].scatter(X_train_pca[:, 0], X_train_pca[:, 1], \n",
    "                          c=y_train, cmap='viridis', \n",
    "                          s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('First Principal Component', fontsize=12)\n",
    "axes[0].set_ylabel('Second Principal Component', fontsize=12)\n",
    "axes[0].set_title('Training Data in PCA Space\\n(Complex interactions not fully visible in 2D)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[0], label='Class')\n",
    "\n",
    "# Plot 2: Feature correlation matrix\n",
    "correlation_matrix = np.corrcoef(X_train.T)\n",
    "im = axes[1].imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[1].set_xlabel('Feature Index', fontsize=12)\n",
    "axes[1].set_ylabel('Feature Index', fontsize=12)\n",
    "axes[1].set_title('Feature Correlation Matrix\\n(Shows feature interaction structure)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[1], label='Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDataset Complexity Indicators:\")\n",
    "print(f\"Explained variance by first 2 PCs: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "print(f\"Average absolute correlation: {np.mean(np.abs(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)])):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5653c",
   "metadata": {},
   "source": [
    "## Part 2: Tree-Based Models\n",
    "\n",
    "We'll test three tree-based architectures with varying capacity:\n",
    "\n",
    "### 1. **Decision Tree** (Single Tree)\n",
    "- **Capacity control**: `max_depth` (2, 5, 10, 15, 20, None)\n",
    "- **Mechanism**: Recursive axis-aligned splits\n",
    "- **Interaction modeling**: Explicit through depth\n",
    "\n",
    "### 2. **Random Forest** (Ensemble of Trees)\n",
    "- **Capacity control**: `n_estimators` and `max_depth`\n",
    "- **Mechanism**: Bootstrap aggregating (bagging)\n",
    "- **Variance reduction**: Through averaging\n",
    "\n",
    "### 3. **Gradient Boosting** (Sequential Ensemble)\n",
    "- **Capacity control**: `n_estimators` and `max_depth`\n",
    "- **Mechanism**: Additive model building\n",
    "- **Bias reduction**: Sequential error correction\n",
    "\n",
    "### Key Properties of Trees:\n",
    "- **Local decisions**: Each split affects only a subset of data\n",
    "- **Greedy learning**: No backpropagation or global optimization\n",
    "- **Hard partitions**: Axis-aligned rectangular regions\n",
    "- **Exponential capacity**: Depth d → 2^d leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c378f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree_models_varying_capacity():\n",
    "    \"\"\"Train tree-based models with different capacity levels\"\"\"\n",
    "    \n",
    "    # Decision Tree: vary depth\n",
    "    dt_capacities = [2, 5, 10, 15, 20, None]\n",
    "    dt_results = []\n",
    "    \n",
    "    print(\"Training Decision Trees...\")\n",
    "    for depth in dt_capacities:\n",
    "        model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "        val_acc = accuracy_score(y_val, model.predict(X_val))\n",
    "        test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "        \n",
    "        # Calculate probabilities for loss\n",
    "        train_proba = model.predict_proba(X_train)\n",
    "        val_proba = model.predict_proba(X_val)\n",
    "        \n",
    "        train_loss = log_loss(y_train, train_proba)\n",
    "        val_loss = log_loss(y_val, val_proba)\n",
    "        \n",
    "        dt_results.append({\n",
    "            'capacity': depth if depth else 'unlimited',\n",
    "            'depth': depth if depth else 20,  # for plotting\n",
    "            'model': model,\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss\n",
    "        })\n",
    "        \n",
    "        print(f\"  Depth {depth}: Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n",
    "    \n",
    "    # Random Forest: vary number of estimators (fixed depth)\n",
    "    rf_capacities = [1, 5, 10, 25, 50, 100, 200]\n",
    "    rf_results = []\n",
    "    \n",
    "    print(\"\\nTraining Random Forests...\")\n",
    "    for n_est in rf_capacities:\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_est, max_depth=10, random_state=42, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "        val_acc = accuracy_score(y_val, model.predict(X_val))\n",
    "        test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "        \n",
    "        train_proba = model.predict_proba(X_train)\n",
    "        val_proba = model.predict_proba(X_val)\n",
    "        \n",
    "        train_loss = log_loss(y_train, train_proba)\n",
    "        val_loss = log_loss(y_val, val_proba)\n",
    "        \n",
    "        rf_results.append({\n",
    "            'capacity': n_est,\n",
    "            'model': model,\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss\n",
    "        })\n",
    "        \n",
    "        print(f\"  n_estimators {n_est}: Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n",
    "    \n",
    "    # Gradient Boosting: vary number of estimators\n",
    "    gb_capacities = [1, 5, 10, 25, 50, 100, 200]\n",
    "    gb_results = []\n",
    "    \n",
    "    print(\"\\nTraining Gradient Boosting...\")\n",
    "    for n_est in gb_capacities:\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=n_est, max_depth=5, learning_rate=0.1, random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "        val_acc = accuracy_score(y_val, model.predict(X_val))\n",
    "        test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "        \n",
    "        train_proba = model.predict_proba(X_train)\n",
    "        val_proba = model.predict_proba(X_val)\n",
    "        \n",
    "        train_loss = log_loss(y_train, train_proba)\n",
    "        val_loss = log_loss(y_val, val_proba)\n",
    "        \n",
    "        gb_results.append({\n",
    "            'capacity': n_est,\n",
    "            'model': model,\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss\n",
    "        })\n",
    "        \n",
    "        print(f\"  n_estimators {n_est}: Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n",
    "    \n",
    "    return dt_results, rf_results, gb_results\n",
    "\n",
    "# Train all tree models\n",
    "dt_results, rf_results, gb_results = train_tree_models_varying_capacity()\n",
    "print(\"\\n✓ Tree-based models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cc8d7",
   "metadata": {},
   "source": [
    "## Part 3: Transformer-Based Models\n",
    "\n",
    "We'll implement transformer architectures for tabular data with varying capacity:\n",
    "\n",
    "### Transformer Components:\n",
    "1. **Input Embedding**: Project features to hidden dimension\n",
    "2. **Multi-Head Self-Attention**: Learn feature interactions\n",
    "3. **Feed-Forward Networks**: Non-linear transformations\n",
    "4. **Layer Normalization**: Stabilize training\n",
    "5. **Classification Head**: Final prediction layer\n",
    "\n",
    "### Capacity Control:\n",
    "- **Hidden dimension** (d_model): 32, 64, 128, 256\n",
    "- **Number of layers**: 1, 2, 4, 6\n",
    "- **Number of attention heads**: 2, 4, 8\n",
    "\n",
    "### Key Properties of Transformers:\n",
    "- **Global context**: Attention sees all features simultaneously\n",
    "- **Soft interactions**: Learned attention weights (not hard splits)\n",
    "- **Gradient-based**: Backpropagation through entire network\n",
    "- **Implicit regularization**: Overparameterization can help generalization\n",
    "- **Polynomial capacity**: Layers × Hidden_dim (more gradual than trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularTransformer(nn.Module):\n",
    "    \"\"\"Transformer architecture for tabular data\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, num_classes, dropout=0.1):\n",
    "        super(TabularTransformer, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding (for feature position awareness)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, input_dim, d_model))\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Expand features for attention: (batch, input_dim) -> (batch, input_dim, 1)\n",
    "        x = x.unsqueeze(-1)  # (batch, input_dim, 1)\n",
    "        \n",
    "        # Project to d_model\n",
    "        x = x.expand(-1, -1, self.d_model)  # (batch, input_dim, d_model)\n",
    "        x = x * self.input_projection.weight.T + self.input_projection.bias\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_embedding\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer_encoder(x)  # (batch, input_dim, d_model)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = torch.mean(x, dim=1)  # (batch, d_model)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the architecture\n",
    "test_model = TabularTransformer(\n",
    "    input_dim=20, d_model=64, nhead=4, num_layers=2, num_classes=3\n",
    ")\n",
    "test_input = torch.randn(10, 20)\n",
    "test_output = test_model(test_input)\n",
    "print(f\"✓ Transformer architecture working: input {test_input.shape} -> output {test_output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Test model parameters: {count_parameters(test_model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6169eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_loader, val_loader, epochs=50, lr=0.001, device='cpu'):\n",
    "    \"\"\"Train a transformer model\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += batch_y.size(0)\n",
    "            train_correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n",
    "                  f\"Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "def train_transformers_varying_capacity():\n",
    "    \"\"\"Train transformers with different capacity levels\"\"\"\n",
    "    \n",
    "    # Prepare data loaders\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Vary hidden dimension (d_model)\n",
    "    transformer_configs = [\n",
    "        {'d_model': 32, 'nhead': 2, 'num_layers': 2, 'name': 'Small (d=32, L=2)'},\n",
    "        {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'name': 'Medium (d=64, L=2)'},\n",
    "        {'d_model': 128, 'nhead': 4, 'num_layers': 2, 'name': 'Large (d=128, L=2)'},\n",
    "        {'d_model': 64, 'nhead': 4, 'num_layers': 1, 'name': 'Shallow (d=64, L=1)'},\n",
    "        {'d_model': 64, 'nhead': 4, 'num_layers': 4, 'name': 'Deep (d=64, L=4)'},\n",
    "        {'d_model': 256, 'nhead': 8, 'num_layers': 4, 'name': 'Very Large (d=256, L=4)'},\n",
    "    ]\n",
    "    \n",
    "    transformer_results = []\n",
    "    \n",
    "    print(\"\\nTraining Transformers...\")\n",
    "    for config in transformer_configs:\n",
    "        print(f\"\\n{config['name']}:\")\n",
    "        \n",
    "        model = TabularTransformer(\n",
    "            input_dim=X_train.shape[1],\n",
    "            d_model=config['d_model'],\n",
    "            nhead=config['nhead'],\n",
    "            num_layers=config['num_layers'],\n",
    "            num_classes=len(np.unique(y_train)),\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        n_params = count_parameters(model)\n",
    "        print(f\"  Parameters: {n_params:,}\")\n",
    "        \n",
    "        # Train\n",
    "        model, train_losses, val_losses, train_accs, val_accs = train_transformer(\n",
    "            model, train_loader, val_loader, epochs=50, lr=0.001, device=device\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += batch_y.size(0)\n",
    "                test_correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        test_acc = test_correct / test_total\n",
    "        \n",
    "        transformer_results.append({\n",
    "            'name': config['name'],\n",
    "            'config': config,\n",
    "            'n_params': n_params,\n",
    "            'model': model,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_accs': train_accs,\n",
    "            'val_accs': val_accs,\n",
    "            'final_train_acc': train_accs[-1],\n",
    "            'final_val_acc': val_accs[-1],\n",
    "            'test_acc': test_acc\n",
    "        })\n",
    "        \n",
    "        print(f\"  Final - Train Acc={train_accs[-1]:.3f}, Val Acc={val_accs[-1]:.3f}, Test Acc={test_acc:.3f}\")\n",
    "    \n",
    "    return transformer_results\n",
    "\n",
    "# Train all transformer models\n",
    "transformer_results = train_transformers_varying_capacity()\n",
    "print(\"\\n✓ Transformer models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6031e",
   "metadata": {},
   "source": [
    "## Part 4: Comparing Capacity Scaling\n",
    "\n",
    "Now let's visualize how bias and variance change with capacity for both model families.\n",
    "\n",
    "### What to Expect:\n",
    "\n",
    "**Tree-Based Models:**\n",
    "- Sharp transition from high bias to high variance\n",
    "- Clear \"sweet spot\" in capacity\n",
    "- Overfitting happens suddenly with too much depth\n",
    "- Ensemble methods smooth this transition\n",
    "\n",
    "**Transformers:**\n",
    "- More gradual capacity scaling\n",
    "- Can benefit from overparameterization\n",
    "- Implicit regularization through optimization\n",
    "- Performance plateau rather than sharp overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# Decision Trees\n",
    "ax = axes[0, 0]\n",
    "depths = [r['depth'] for r in dt_results]\n",
    "train_accs = [r['train_acc'] for r in dt_results]\n",
    "val_accs = [r['val_acc'] for r in dt_results]\n",
    "test_accs = [r['test_acc'] for r in dt_results]\n",
    "\n",
    "ax.plot(depths, train_accs, 'o-', linewidth=2, markersize=8, label='Train', color='blue')\n",
    "ax.plot(depths, val_accs, 's-', linewidth=2, markersize=8, label='Validation', color='orange')\n",
    "ax.plot(depths, test_accs, '^-', linewidth=2, markersize=8, label='Test', color='green')\n",
    "ax.set_xlabel('Max Depth', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Decision Tree: Capacity vs Performance', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(depths)\n",
    "\n",
    "# Add overfitting annotation\n",
    "max_depth_idx = np.argmax(val_accs)\n",
    "ax.axvline(depths[max_depth_idx], color='red', linestyle='--', alpha=0.5, linewidth=2)\n",
    "ax.text(depths[max_depth_idx], 0.5, 'Optimal\\nCapacity', \n",
    "        rotation=90, verticalalignment='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "# Random Forest\n",
    "ax = axes[0, 1]\n",
    "n_ests = [r['capacity'] for r in rf_results]\n",
    "train_accs = [r['train_acc'] for r in rf_results]\n",
    "val_accs = [r['val_acc'] for r in rf_results]\n",
    "test_accs = [r['test_acc'] for r in rf_results]\n",
    "\n",
    "ax.plot(n_ests, train_accs, 'o-', linewidth=2, markersize=8, label='Train', color='blue')\n",
    "ax.plot(n_ests, val_accs, 's-', linewidth=2, markersize=8, label='Validation', color='orange')\n",
    "ax.plot(n_ests, test_accs, '^-', linewidth=2, markersize=8, label='Test', color='green')\n",
    "ax.set_xlabel('Number of Trees', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Random Forest: Capacity vs Performance', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Gradient Boosting\n",
    "ax = axes[0, 2]\n",
    "n_ests = [r['capacity'] for r in gb_results]\n",
    "train_accs = [r['train_acc'] for r in gb_results]\n",
    "val_accs = [r['val_acc'] for r in gb_results]\n",
    "test_accs = [r['test_acc'] for r in gb_results]\n",
    "\n",
    "ax.plot(n_ests, train_accs, 'o-', linewidth=2, markersize=8, label='Train', color='blue')\n",
    "ax.plot(n_ests, val_accs, 's-', linewidth=2, markersize=8, label='Validation', color='orange')\n",
    "ax.plot(n_ests, test_accs, '^-', linewidth=2, markersize=8, label='Test', color='green')\n",
    "ax.set_xlabel('Number of Boosting Rounds', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Gradient Boosting: Capacity vs Performance', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Generalization gap (overfitting indicator)\n",
    "ax = axes[1, 0]\n",
    "gaps_dt = [train - val for train, val in zip([r['train_acc'] for r in dt_results], \n",
    "                                               [r['val_acc'] for r in dt_results])]\n",
    "ax.plot(depths, gaps_dt, 'o-', linewidth=2.5, markersize=10, color='red', label='Decision Tree')\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.fill_between(depths, 0, gaps_dt, alpha=0.3, color='red')\n",
    "ax.set_xlabel('Max Depth', fontsize=12)\n",
    "ax.set_ylabel('Train Acc - Val Acc (Overfitting)', fontsize=12)\n",
    "ax.set_title('Decision Tree: Overfitting vs Capacity', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(depths)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "gaps_rf = [train - val for train, val in zip([r['train_acc'] for r in rf_results], \n",
    "                                               [r['val_acc'] for r in rf_results])]\n",
    "ax.plot(n_ests, gaps_rf, 's-', linewidth=2.5, markersize=10, color='purple', label='Random Forest')\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.fill_between([r['capacity'] for r in rf_results], 0, gaps_rf, alpha=0.3, color='purple')\n",
    "ax.set_xlabel('Number of Trees', fontsize=12)\n",
    "ax.set_ylabel('Train Acc - Val Acc (Overfitting)', fontsize=12)\n",
    "ax.set_title('Random Forest: Overfitting vs Capacity', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "ax = axes[1, 2]\n",
    "gaps_gb = [train - val for train, val in zip([r['train_acc'] for r in gb_results], \n",
    "                                               [r['val_acc'] for r in gb_results])]\n",
    "ax.plot([r['capacity'] for r in gb_results], gaps_gb, '^-', linewidth=2.5, markersize=10, \n",
    "        color='darkgreen', label='Gradient Boosting')\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.fill_between([r['capacity'] for r in gb_results], 0, gaps_gb, alpha=0.3, color='darkgreen')\n",
    "ax.set_xlabel('Number of Boosting Rounds', fontsize=12)\n",
    "ax.set_ylabel('Train Acc - Val Acc (Overfitting)', fontsize=12)\n",
    "ax.set_title('Gradient Boosting: Overfitting vs Capacity', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.suptitle('Tree-Based Models: Capacity Scaling Analysis', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations for Trees:\")\n",
    "print(\"1. Decision Trees show SHARP transition from underfitting to overfitting\")\n",
    "print(\"2. Random Forest smooths this transition through ensemble averaging\")\n",
    "print(\"3. Gradient Boosting shows gradual improvement with diminishing returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc03eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Performance vs Parameters\n",
    "ax = axes[0, 0]\n",
    "n_params = [r['n_params'] for r in transformer_results]\n",
    "final_train = [r['final_train_acc'] for r in transformer_results]\n",
    "final_val = [r['final_val_acc'] for r in transformer_results]\n",
    "test_acc = [r['test_acc'] for r in transformer_results]\n",
    "\n",
    "ax.plot(n_params, final_train, 'o-', linewidth=2, markersize=10, label='Train', color='blue')\n",
    "ax.plot(n_params, final_val, 's-', linewidth=2, markersize=10, label='Validation', color='orange')\n",
    "ax.plot(n_params, test_acc, '^-', linewidth=2, markersize=10, label='Test', color='green')\n",
    "ax.set_xlabel('Number of Parameters', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Transformer: Model Size vs Performance', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Add annotations for each model\n",
    "for i, r in enumerate(transformer_results):\n",
    "    ax.annotate(r['name'].split('(')[0].strip(), \n",
    "                xy=(r['n_params'], r['final_val_acc']),\n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=8, alpha=0.7)\n",
    "\n",
    "# Overfitting gap\n",
    "ax = axes[0, 1]\n",
    "gaps = [train - val for train, val in zip(final_train, final_val)]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(transformer_results)))\n",
    "\n",
    "bars = ax.bar(range(len(transformer_results)), gaps, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_xticks(range(len(transformer_results)))\n",
    "ax.set_xticklabels([r['name'].split('(')[0].strip() for r in transformer_results], \n",
    "                    rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel('Train Acc - Val Acc (Overfitting)', fontsize=12)\n",
    "ax.set_title('Transformer: Overfitting vs Model Configuration', fontsize=13, fontweight='bold')\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Training curves for different capacities\n",
    "ax = axes[1, 0]\n",
    "for i, r in enumerate(transformer_results):\n",
    "    if i in [0, 2, 5]:  # Select small, medium, very large\n",
    "        ax.plot(r['val_losses'], linewidth=2, label=r['name'], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax.set_title('Transformer: Training Dynamics by Capacity', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Final comparison\n",
    "ax = axes[1, 1]\n",
    "x_pos = np.arange(len(transformer_results))\n",
    "width = 0.35\n",
    "\n",
    "final_train_accs = [r['final_train_acc'] for r in transformer_results]\n",
    "final_val_accs = [r['final_val_acc'] for r in transformer_results]\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, final_train_accs, width, label='Train', \n",
    "               color='blue', alpha=0.7, edgecolor='black')\n",
    "bars2 = ax.bar(x_pos + width/2, final_val_accs, width, label='Validation', \n",
    "               color='orange', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Transformer: Train vs Validation Accuracy', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([r['name'].split('(')[0].strip() for r in transformer_results], \n",
    "                    rotation=45, ha='right', fontsize=10)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Transformer Models: Capacity Scaling Analysis', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations for Transformers:\")\n",
    "print(\"1. More GRADUAL scaling with capacity\")\n",
    "print(\"2. Larger models can generalize well despite overparameterization\")\n",
    "print(\"3. Training dynamics show smooth convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf809969",
   "metadata": {},
   "source": [
    "## Part 5: Direct Comparison: Trees vs Transformers\n",
    "\n",
    "Let's directly compare how these two model families respond to increasing capacity.\n",
    "\n",
    "### Key Metrics:\n",
    "1. **Capacity** (normalized for comparison)\n",
    "2. **Bias**: Distance from optimal performance\n",
    "3. **Variance**: Generalization gap (train - val accuracy)\n",
    "4. **Scaling behavior**: How quickly performance improves with capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e9b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Normalize capacity for comparison\n",
    "# For trees: use depth or number of estimators\n",
    "# For transformers: use number of parameters\n",
    "\n",
    "# Plot 1: Bias (measured as 1 - validation accuracy)\n",
    "ax = axes[0, 0]\n",
    "\n",
    "# Decision Trees\n",
    "dt_depths = [r['depth'] for r in dt_results]\n",
    "dt_bias = [1 - r['val_acc'] for r in dt_results]\n",
    "ax.plot(dt_depths, dt_bias, 'o-', linewidth=2.5, markersize=10, \n",
    "        label='Decision Tree', color='brown', alpha=0.8)\n",
    "\n",
    "# Random Forest (normalize n_estimators to similar scale)\n",
    "rf_n_est = [r['capacity'] for r in rf_results]\n",
    "rf_bias = [1 - r['val_acc'] for r in rf_results]\n",
    "rf_normalized = [x / 10 for x in rf_n_est]  # Normalize for comparison\n",
    "ax.plot(rf_normalized, rf_bias, 's-', linewidth=2.5, markersize=10, \n",
    "        label='Random Forest', color='green', alpha=0.8)\n",
    "\n",
    "# Transformers (normalize params to similar scale)\n",
    "trans_params = [r['n_params'] / 1000 for r in transformer_results]  # Scale to thousands\n",
    "trans_bias = [1 - r['final_val_acc'] for r in transformer_results]\n",
    "ax.plot(trans_params, trans_bias, '^-', linewidth=2.5, markersize=10, \n",
    "        label='Transformer', color='purple', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Relative Capacity (normalized)', fontsize=12)\n",
    "ax.set_ylabel('Bias (1 - Validation Accuracy)', fontsize=12)\n",
    "ax.set_title('Bias Reduction with Increasing Capacity', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 2: Variance (generalization gap)\n",
    "ax = axes[0, 1]\n",
    "\n",
    "dt_variance = [r['train_acc'] - r['val_acc'] for r in dt_results]\n",
    "ax.plot(dt_depths, dt_variance, 'o-', linewidth=2.5, markersize=10, \n",
    "        label='Decision Tree', color='brown', alpha=0.8)\n",
    "\n",
    "rf_variance = [r['train_acc'] - r['val_acc'] for r in rf_results]\n",
    "ax.plot(rf_normalized, rf_variance, 's-', linewidth=2.5, markersize=10, \n",
    "        label='Random Forest', color='green', alpha=0.8)\n",
    "\n",
    "trans_variance = [r['final_train_acc'] - r['final_val_acc'] for r in transformer_results]\n",
    "ax.plot(trans_params, trans_variance, '^-', linewidth=2.5, markersize=10, \n",
    "        label='Transformer', color='purple', alpha=0.8)\n",
    "\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Relative Capacity (normalized)', fontsize=12)\n",
    "ax.set_ylabel('Variance (Train - Val Accuracy)', fontsize=12)\n",
    "ax.set_title('Variance Increase with Capacity', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Shade \"danger zone\" for high variance\n",
    "ax.axhspan(0.1, 0.5, alpha=0.1, color='red', label='High Variance Zone')\n",
    "\n",
    "# Plot 3: Bias-Variance Tradeoff Space\n",
    "ax = axes[1, 0]\n",
    "\n",
    "# Decision Tree trajectory\n",
    "ax.plot(dt_bias, dt_variance, 'o-', linewidth=2.5, markersize=10, \n",
    "        label='Decision Tree', color='brown', alpha=0.8)\n",
    "for i, depth in enumerate(dt_depths[::2]):  # Annotate every other point\n",
    "    if i * 2 < len(dt_depths):\n",
    "        ax.annotate(f'd={dt_depths[i*2]}', \n",
    "                    xy=(dt_bias[i*2], dt_variance[i*2]),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8, alpha=0.7)\n",
    "\n",
    "# Random Forest trajectory\n",
    "ax.plot(rf_bias, rf_variance, 's-', linewidth=2.5, markersize=10, \n",
    "        label='Random Forest', color='green', alpha=0.8)\n",
    "\n",
    "# Transformer trajectory\n",
    "ax.plot(trans_bias, trans_variance, '^-', linewidth=2.5, markersize=10, \n",
    "        label='Transformer', color='purple', alpha=0.8)\n",
    "\n",
    "# Mark optimal region\n",
    "optimal_region = plt.Circle((0.15, 0.02), 0.05, color='gold', alpha=0.2)\n",
    "ax.add_patch(optimal_region)\n",
    "ax.text(0.15, 0.02, 'Optimal\\nRegion', ha='center', va='center', \n",
    "        fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Bias (1 - Val Accuracy)', fontsize=12)\n",
    "ax.set_ylabel('Variance (Train - Val Accuracy)', fontsize=12)\n",
    "ax.set_title('Bias-Variance Tradeoff Space', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add arrows showing trajectory direction\n",
    "ax.annotate('', xy=(dt_bias[-1], dt_variance[-1]), \n",
    "            xytext=(dt_bias[0], dt_variance[0]),\n",
    "            arrowprops=dict(arrowstyle='->', lw=1.5, color='brown', alpha=0.3))\n",
    "ax.annotate('', xy=(trans_bias[-1], trans_variance[-1]), \n",
    "            xytext=(trans_bias[0], trans_variance[0]),\n",
    "            arrowprops=dict(arrowstyle='->', lw=1.5, color='purple', alpha=0.3))\n",
    "\n",
    "# Plot 4: Scaling Efficiency\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# How much does validation accuracy improve per unit of capacity?\n",
    "dt_capacity_normalized = np.array(dt_depths) / max(dt_depths)\n",
    "dt_val_accs = [r['val_acc'] for r in dt_results]\n",
    "\n",
    "rf_capacity_normalized = np.array(rf_n_est) / max(rf_n_est)\n",
    "rf_val_accs = [r['val_acc'] for r in rf_results]\n",
    "\n",
    "trans_capacity_normalized = np.array([r['n_params'] for r in transformer_results]) / max([r['n_params'] for r in transformer_results])\n",
    "trans_val_accs = [r['final_val_acc'] for r in transformer_results]\n",
    "\n",
    "ax.plot(dt_capacity_normalized, dt_val_accs, 'o-', linewidth=2.5, markersize=10, \n",
    "        label='Decision Tree', color='brown', alpha=0.8)\n",
    "ax.plot(rf_capacity_normalized, rf_val_accs, 's-', linewidth=2.5, markersize=10, \n",
    "        label='Random Forest', color='green', alpha=0.8)\n",
    "ax.plot(trans_capacity_normalized, trans_val_accs, '^-', linewidth=2.5, markersize=10, \n",
    "        label='Transformer', color='purple', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Normalized Capacity (0-1)', fontsize=12)\n",
    "ax.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax.set_title('Scaling Efficiency: Performance vs Capacity', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add reference lines\n",
    "ax.axhline(max(dt_val_accs + rf_val_accs + trans_val_accs), \n",
    "           color='gold', linestyle='--', alpha=0.5, linewidth=2, label='Best Performance')\n",
    "\n",
    "plt.suptitle('Direct Comparison: Trees vs Transformers', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59176a4",
   "metadata": {},
   "source": [
    "## Part 6: Empirical Bias-Variance Decomposition\n",
    "\n",
    "To truly measure bias and variance, we need to:\n",
    "1. Train multiple models on different bootstrap samples\n",
    "2. Calculate prediction variance across models (variance)\n",
    "3. Calculate average prediction error (bias²)\n",
    "\n",
    "This is the classic **bias-variance decomposition** experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eccc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_bias_variance_decomposition(model_class, model_params, X_train, y_train, \n",
    "                                          X_test, y_test, n_bootstrap=20):\n",
    "    \"\"\"\n",
    "    Perform empirical bias-variance decomposition.\n",
    "    Train multiple models on bootstrap samples and measure bias/variance.\n",
    "    \"\"\"\n",
    "    n_samples = len(X_train)\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Training {n_bootstrap} models on bootstrap samples...\")\n",
    "    for i in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_boot = X_train[indices]\n",
    "        y_boot = y_train[indices]\n",
    "        \n",
    "        # Train model\n",
    "        if 'transformer' in str(model_class).lower():\n",
    "            # For transformer, need special handling\n",
    "            # Skip for now or implement if needed\n",
    "            continue\n",
    "        else:\n",
    "            model = model_class(**model_params)\n",
    "            model.fit(X_boot, y_boot)\n",
    "        \n",
    "        # Predict on test set\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            pred = model.predict_proba(X_test)\n",
    "        else:\n",
    "            pred = model.predict(X_test)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)  # shape: (n_bootstrap, n_test, n_classes)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # Average prediction across all models\n",
    "    if len(predictions.shape) == 3:  # Probabilities\n",
    "        mean_predictions = np.mean(predictions, axis=0)\n",
    "        # Variance: variance of predictions across models\n",
    "        variance = np.mean(np.var(predictions, axis=0))\n",
    "    else:  # Hard predictions\n",
    "        mean_predictions = np.mean(predictions, axis=0)\n",
    "        variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    # Bias: error of average prediction\n",
    "    # Convert to class predictions\n",
    "    if len(predictions.shape) == 3:\n",
    "        avg_pred_class = np.argmax(mean_predictions, axis=1)\n",
    "    else:\n",
    "        avg_pred_class = np.round(mean_predictions).astype(int)\n",
    "    \n",
    "    bias_squared = np.mean(avg_pred_class != y_test)\n",
    "    \n",
    "    return bias_squared, variance, predictions\n",
    "\n",
    "# Compare Decision Tree at different depths\n",
    "print(\"=\"*70)\n",
    "print(\"EMPIRICAL BIAS-VARIANCE DECOMPOSITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "depths_to_test = [3, 10, None]\n",
    "dt_bias_variance = []\n",
    "\n",
    "for depth in depths_to_test:\n",
    "    print(f\"\\nDecision Tree (depth={depth}):\")\n",
    "    bias_sq, variance, _ = empirical_bias_variance_decomposition(\n",
    "        DecisionTreeClassifier,\n",
    "        {'max_depth': depth, 'random_state': 42},\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        n_bootstrap=20\n",
    "    )\n",
    "    dt_bias_variance.append({\n",
    "        'depth': depth if depth else 'unlimited',\n",
    "        'bias_squared': bias_sq,\n",
    "        'variance': variance\n",
    "    })\n",
    "    print(f\"  Bias² (error rate): {bias_sq:.4f}\")\n",
    "    print(f\"  Variance: {variance:.4f}\")\n",
    "\n",
    "# Compare Random Forest with different n_estimators\n",
    "rf_n_ests = [10, 50, 200]\n",
    "rf_bias_variance = []\n",
    "\n",
    "for n_est in rf_n_ests:\n",
    "    print(f\"\\nRandom Forest (n_estimators={n_est}):\")\n",
    "    bias_sq, variance, _ = empirical_bias_variance_decomposition(\n",
    "        RandomForestClassifier,\n",
    "        {'n_estimators': n_est, 'max_depth': 10, 'random_state': 42, 'n_jobs': -1},\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        n_bootstrap=20\n",
    "    )\n",
    "    rf_bias_variance.append({\n",
    "        'n_estimators': n_est,\n",
    "        'bias_squared': bias_sq,\n",
    "        'variance': variance\n",
    "    })\n",
    "    print(f\"  Bias² (error rate): {bias_sq:.4f}\")\n",
    "    print(f\"  Variance: {variance:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Decision Tree\n",
    "ax = axes[0]\n",
    "depths_plot = [str(d['depth']) for d in dt_bias_variance]\n",
    "bias_vals = [d['bias_squared'] for d in dt_bias_variance]\n",
    "var_vals = [d['variance'] for d in dt_bias_variance]\n",
    "\n",
    "x_pos = np.arange(len(depths_plot))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, bias_vals, width, label='Bias²', \n",
    "               color='steelblue', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax.bar(x_pos + width/2, var_vals, width, label='Variance', \n",
    "               color='coral', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Error Magnitude', fontsize=12)\n",
    "ax.set_xlabel('Max Depth', fontsize=12)\n",
    "ax.set_title('Decision Tree: Empirical Bias-Variance Decomposition', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(depths_plot)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Random Forest\n",
    "ax = axes[1]\n",
    "n_ests_plot = [str(d['n_estimators']) for d in rf_bias_variance]\n",
    "bias_vals = [d['bias_squared'] for d in rf_bias_variance]\n",
    "var_vals = [d['variance'] for d in rf_bias_variance]\n",
    "\n",
    "x_pos = np.arange(len(n_ests_plot))\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, bias_vals, width, label='Bias²', \n",
    "               color='steelblue', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax.bar(x_pos + width/2, var_vals, width, label='Variance', \n",
    "               color='coral', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Error Magnitude', fontsize=12)\n",
    "ax.set_xlabel('Number of Trees', fontsize=12)\n",
    "ax.set_title('Random Forest: Empirical Bias-Variance Decomposition', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(n_ests_plot)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae10066",
   "metadata": {},
   "source": [
    "## Part 7: Explaining the Fundamental Differences\n",
    "\n",
    "### Why do Transformers and Trees respond differently to capacity increases?\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Learning Mechanism**\n",
    "\n",
    "### Trees (Greedy, Local):\n",
    "- **Greedy splitting**: Each split optimizes locally, not globally\n",
    "- **No gradient flow**: Can't backpropagate to fix earlier mistakes\n",
    "- **Hard partitions**: Data points are definitively assigned to regions\n",
    "- **Result**: Need exponentially more capacity (depth) to model interactions\n",
    "\n",
    "### Transformers (Gradient-based, Global):\n",
    "- **End-to-end optimization**: Gradients flow through entire network\n",
    "- **Soft attention**: Probabilistic combination of all features\n",
    "- **Continuous representations**: Smooth, differentiable functions\n",
    "- **Result**: Can efficiently model complex interactions with modest capacity\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Capacity Scaling**\n",
    "\n",
    "### Trees:\n",
    "```\n",
    "Capacity ≈ 2^depth  (exponential)\n",
    "```\n",
    "- Doubling depth quadruples capacity\n",
    "- Each level doubles possible regions\n",
    "- **Sharp transition**: Small depth change = huge capacity change\n",
    "\n",
    "### Transformers:\n",
    "```\n",
    "Capacity ≈ layers × d_model²  (polynomial)\n",
    "```\n",
    "- Linear scaling with depth\n",
    "- Quadratic scaling with width\n",
    "- **Gradual transition**: Small changes = small capacity changes\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Implicit Regularization**\n",
    "\n",
    "### Trees:\n",
    "- **No implicit regularization** (except ensemble averaging)\n",
    "- Need explicit constraints (max_depth, min_samples_split)\n",
    "- Overfitting happens suddenly when constraints are relaxed\n",
    "\n",
    "### Transformers:\n",
    "- **SGD implicit bias**: Gradient descent finds flat minima\n",
    "- **Overparameterization helps**: More parameters → better optimization landscape\n",
    "- **Early stopping**: Training dynamics naturally regulate\n",
    "- Can benefit from models larger than strictly necessary\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Feature Interactions**\n",
    "\n",
    "### Trees:\n",
    "- **Hierarchical splits**: Feature A at level 1, Feature B at level 2\n",
    "- **Order matters**: Different orderings create different models\n",
    "- **Limited interaction**: Depth d → can model d-way interactions\n",
    "- **Example**: To learn X₁ × X₂ × X₃, need depth ≥ 3\n",
    "\n",
    "### Transformers:\n",
    "- **Self-attention**: All features interact simultaneously\n",
    "- **Order-invariant**: (with positional encoding, order is added separately)\n",
    "- **Rich interactions**: Single layer can model complex feature combinations\n",
    "- **Example**: Can learn X₁ × X₂ × X₃ in one attention layer\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Variance Sources**\n",
    "\n",
    "### Trees - High Variance from:\n",
    "1. **Bootstrap sensitivity**: Different samples → different split points\n",
    "2. **High sensitivity**: Small data change → different tree structure\n",
    "3. **Discrete decisions**: Binary splits are unstable\n",
    "4. **Mitigation**: Ensemble methods (bagging, boosting)\n",
    "\n",
    "### Transformers - Lower Variance from:\n",
    "1. **Continuous optimization**: Smooth loss landscape\n",
    "2. **Regularization**: Dropout, weight decay, gradient noise\n",
    "3. **Soft decisions**: Attention weights are continuous\n",
    "4. **Parameter sharing**: Same weights used everywhere\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **The Capacity Paradox**\n",
    "\n",
    "### Trees:\n",
    "- ❌ **More capacity** (depth) → immediate overfitting\n",
    "- ❌ **More trees** → diminishing returns past ~100-200 trees\n",
    "- ✅ **Sweet spot**: Moderate depth (5-10) with ensemble\n",
    "\n",
    "### Transformers:\n",
    "- ✅ **More capacity** → often better, even if \"overparameterized\"\n",
    "- ✅ **More layers** → can still generalize with proper training\n",
    "- ✅ **Modern regime**: Bigger models trained with good practices generalize better\n",
    "\n",
    "This is the **double descent** phenomenon: \n",
    "- Classical regime: more capacity → overfitting\n",
    "- Modern regime: even more capacity → good generalization again\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Aspect | Trees | Transformers |\n",
    "|--------|-------|--------------|\n",
    "| **Capacity Scaling** | Exponential (2^depth) | Polynomial (layers × d²) |\n",
    "| **Learning** | Greedy, local | Gradient-based, global |\n",
    "| **Interactions** | Hierarchical, limited | Simultaneous, rich |\n",
    "| **Variance** | High (discrete splits) | Lower (continuous) |\n",
    "| **Regularization** | Explicit only | Implicit + explicit |\n",
    "| **Transition** | Sharp (cliff) | Gradual (slope) |\n",
    "| **Overparameterization** | Always bad | Can be good |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary figure\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# Row 1: Capacity curves comparison\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "\n",
    "# Normalize all capacities to 0-1 range for comparison\n",
    "dt_cap_norm = np.array([r['depth'] for r in dt_results]) / max([r['depth'] for r in dt_results])\n",
    "dt_val = [r['val_acc'] for r in dt_results]\n",
    "dt_train = [r['train_acc'] for r in dt_results]\n",
    "\n",
    "rf_cap_norm = np.array([r['capacity'] for r in rf_results]) / max([r['capacity'] for r in rf_results])\n",
    "rf_val = [r['val_acc'] for r in rf_results]\n",
    "rf_train = [r['train_acc'] for r in rf_results]\n",
    "\n",
    "trans_cap_norm = np.array([r['n_params'] for r in transformer_results]) / max([r['n_params'] for r in transformer_results])\n",
    "trans_val = [r['final_val_acc'] for r in transformer_results]\n",
    "trans_train = [r['final_train_acc'] for r in transformer_results]\n",
    "\n",
    "# Plot training curves\n",
    "ax1.plot(dt_cap_norm, dt_train, 'o--', linewidth=2, markersize=8, \n",
    "         label='Decision Tree (Train)', color='brown', alpha=0.5)\n",
    "ax1.plot(dt_cap_norm, dt_val, 'o-', linewidth=2.5, markersize=10, \n",
    "         label='Decision Tree (Val)', color='brown', alpha=0.9)\n",
    "\n",
    "ax1.plot(rf_cap_norm, rf_train, 's--', linewidth=2, markersize=8, \n",
    "         label='Random Forest (Train)', color='green', alpha=0.5)\n",
    "ax1.plot(rf_cap_norm, rf_val, 's-', linewidth=2.5, markersize=10, \n",
    "         label='Random Forest (Val)', color='green', alpha=0.9)\n",
    "\n",
    "ax1.plot(trans_cap_norm, trans_train, '^--', linewidth=2, markersize=8, \n",
    "         label='Transformer (Train)', color='purple', alpha=0.5)\n",
    "ax1.plot(trans_cap_norm, trans_val, '^-', linewidth=2.5, markersize=10, \n",
    "         label='Transformer (Val)', color='purple', alpha=0.9)\n",
    "\n",
    "ax1.set_xlabel('Normalized Capacity (0 = minimum, 1 = maximum)', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Model Family Comparison: How Performance Scales with Capacity', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10, ncol=3, loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "ax1.annotate('Sharp\\ntransition', xy=(0.3, 0.8), xytext=(0.3, 0.65),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='brown'),\n",
    "            fontsize=11, ha='center', color='brown', fontweight='bold')\n",
    "\n",
    "ax1.annotate('Gradual\\nimprovement', xy=(0.7, 0.72), xytext=(0.7, 0.60),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='purple'),\n",
    "            fontsize=11, ha='center', color='purple', fontweight='bold')\n",
    "\n",
    "# Row 2: Overfitting patterns\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "dt_gaps = [r['train_acc'] - r['val_acc'] for r in dt_results]\n",
    "ax2.fill_between(dt_cap_norm, 0, dt_gaps, alpha=0.5, color='brown')\n",
    "ax2.plot(dt_cap_norm, dt_gaps, 'o-', linewidth=2.5, markersize=10, color='brown')\n",
    "ax2.set_title('Decision Tree\\nOverfitting Pattern', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Normalized Capacity', fontsize=11)\n",
    "ax2.set_ylabel('Train - Val Accuracy', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax2.text(0.5, max(dt_gaps)*0.8, 'Sharp\\nincrease', fontsize=10, ha='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "rf_gaps = [r['train_acc'] - r['val_acc'] for r in rf_results]\n",
    "ax3.fill_between(rf_cap_norm, 0, rf_gaps, alpha=0.5, color='green')\n",
    "ax3.plot(rf_cap_norm, rf_gaps, 's-', linewidth=2.5, markersize=10, color='green')\n",
    "ax3.set_title('Random Forest\\nOverfitting Pattern', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Normalized Capacity', fontsize=11)\n",
    "ax3.set_ylabel('Train - Val Accuracy', fontsize=11)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax3.text(0.5, max(rf_gaps)*0.8, 'Controlled\\nby ensemble', fontsize=10, ha='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "trans_gaps = [r['final_train_acc'] - r['final_val_acc'] for r in transformer_results]\n",
    "ax4.fill_between(trans_cap_norm, 0, trans_gaps, alpha=0.5, color='purple')\n",
    "ax4.plot(trans_cap_norm, trans_gaps, '^-', linewidth=2.5, markersize=10, color='purple')\n",
    "ax4.set_title('Transformer\\nOverfitting Pattern', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Normalized Capacity', fontsize=11)\n",
    "ax4.set_ylabel('Train - Val Accuracy', fontsize=11)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax4.text(0.5, max(trans_gaps)*0.8, 'Minimal\\noverfitting', fontsize=10, ha='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='lavender', alpha=0.7))\n",
    "\n",
    "# Row 3: Summary statistics\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "\n",
    "# Create summary table data\n",
    "model_types = ['Decision Tree\\n(max capacity)', 'Random Forest\\n(max capacity)', \n",
    "               'Transformer\\n(max capacity)']\n",
    "\n",
    "best_val_accs = [\n",
    "    max([r['val_acc'] for r in dt_results]),\n",
    "    max([r['val_acc'] for r in rf_results]),\n",
    "    max([r['final_val_acc'] for r in transformer_results])\n",
    "]\n",
    "\n",
    "min_overfit_gaps = [\n",
    "    min([abs(r['train_acc'] - r['val_acc']) for r in dt_results]),\n",
    "    min([abs(r['train_acc'] - r['val_acc']) for r in rf_results]),\n",
    "    min([abs(r['final_train_acc'] - r['final_val_acc']) for r in transformer_results])\n",
    "]\n",
    "\n",
    "max_overfit_gaps = [\n",
    "    max([abs(r['train_acc'] - r['val_acc']) for r in dt_results]),\n",
    "    max([abs(r['train_acc'] - r['val_acc']) for r in rf_results]),\n",
    "    max([abs(r['final_train_acc'] - r['final_val_acc']) for r in transformer_results])\n",
    "]\n",
    "\n",
    "x = np.arange(len(model_types))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax5.bar(x - width, best_val_accs, width, label='Best Val Accuracy', \n",
    "                color='gold', alpha=0.8, edgecolor='black', linewidth=2)\n",
    "bars2 = ax5.bar(x, min_overfit_gaps, width, label='Min Overfitting Gap', \n",
    "                color='lightgreen', alpha=0.8, edgecolor='black', linewidth=2)\n",
    "bars3 = ax5.bar(x + width, max_overfit_gaps, width, label='Max Overfitting Gap', \n",
    "                color='salmon', alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax5.set_ylabel('Value', fontsize=13, fontweight='bold')\n",
    "ax5.set_title('Summary: Peak Performance and Overfitting Range', fontsize=14, fontweight='bold')\n",
    "ax5.set_xticks(x)\n",
    "ax5.set_xticklabels(model_types, fontsize=11)\n",
    "ax5.legend(fontsize=11, loc='upper left')\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.suptitle('COMPREHENSIVE COMPARISON: TREES vs TRANSFORMERS\\nCapacity Scaling and Bias-Variance Tradeoff', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBest Validation Accuracy:\")\n",
    "print(f\"  Decision Tree: {max([r['val_acc'] for r in dt_results]):.4f}\")\n",
    "print(f\"  Random Forest: {max([r['val_acc'] for r in rf_results]):.4f}\")\n",
    "print(f\"  Transformer:   {max([r['final_val_acc'] for r in transformer_results]):.4f}\")\n",
    "\n",
    "print(\"\\nOverfitting Behavior:\")\n",
    "print(f\"  Decision Tree: Overfitting gap ranges from {min_overfit_gaps[0]:.4f} to {max_overfit_gaps[0]:.4f}\")\n",
    "print(f\"  Random Forest: Overfitting gap ranges from {min_overfit_gaps[1]:.4f} to {max_overfit_gaps[1]:.4f}\")\n",
    "print(f\"  Transformer:   Overfitting gap ranges from {min_overfit_gaps[2]:.4f} to {max_overfit_gaps[2]:.4f}\")\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"  • Trees show SHARP transition: low capacity (high bias) → high capacity (high variance)\")\n",
    "print(\"  • Transformers show GRADUAL scaling: can handle high capacity without severe overfitting\")\n",
    "print(\"  • Ensembles (RF) smooth the transition but still show the sharp pattern\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e654a4",
   "metadata": {},
   "source": [
    "## Conclusions: Why the Tradeoff is Different\n",
    "\n",
    "### Summary of Findings:\n",
    "\n",
    "1. **Capacity Scaling**\n",
    "   - **Trees**: Exponential capacity growth (2^depth) leads to sharp overfitting\n",
    "   - **Transformers**: Polynomial capacity growth allows gradual scaling\n",
    "   \n",
    "2. **Learning Dynamics**\n",
    "   - **Trees**: Greedy, local decisions without global optimization\n",
    "   - **Transformers**: End-to-end gradient optimization with implicit regularization\n",
    "\n",
    "3. **Bias-Variance Behavior**\n",
    "   - **Trees**: Classic U-shaped curve - clear sweet spot\n",
    "   - **Transformers**: More forgiving - larger models can still generalize\n",
    "\n",
    "4. **Feature Interactions**\n",
    "   - **Trees**: Limited by depth, hierarchical interactions\n",
    "   - **Transformers**: Rich interactions through attention, all features interact\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implications:\n",
    "\n",
    "### When to Use Trees:\n",
    "✅ **Tabular data with moderate complexity**\n",
    "✅ **Interpretability is crucial**\n",
    "✅ **Limited computational resources**\n",
    "✅ **Small to medium datasets**\n",
    "✅ **Feature importance analysis needed**\n",
    "\n",
    "**Best Practice**: Use ensemble methods (Random Forest, GradientBoosting) with moderate depth\n",
    "\n",
    "### When to Use Transformers:\n",
    "✅ **Complex feature interactions**\n",
    "✅ **Large datasets available**\n",
    "✅ **Computational resources available**\n",
    "✅ **Sequential or relational data**\n",
    "✅ **Transfer learning beneficial**\n",
    "\n",
    "**Best Practice**: Start with moderate size, scale up if needed, use proper regularization\n",
    "\n",
    "---\n",
    "\n",
    "## The Modern Paradigm Shift:\n",
    "\n",
    "**Classical ML (Trees)**:\n",
    "```\n",
    "More parameters → Overfitting (always bad)\n",
    "Sweet spot at moderate complexity\n",
    "```\n",
    "\n",
    "**Modern Deep Learning (Transformers)**:\n",
    "```\n",
    "More parameters → Better optimization → Implicit regularization\n",
    "Can benefit from overparameterization\n",
    "Double descent phenomenon\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaway:\n",
    "\n",
    "The bias-variance tradeoff manifests differently because:\n",
    "- **Architecture**: How models represent functions\n",
    "- **Optimization**: How models learn from data\n",
    "- **Capacity scaling**: How complexity grows with parameters\n",
    "- **Regularization**: How models prevent overfitting\n",
    "\n",
    "Understanding these differences helps you:\n",
    "1. Choose the right model family for your problem\n",
    "2. Set appropriate hyperparameters\n",
    "3. Diagnose and fix performance issues\n",
    "4. Know when to scale up vs regularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2030f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
