{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5689395f",
   "metadata": {},
   "source": [
    "# High Train + High Validation Error: Data-Centric vs Model-Centric Experiments\n",
    "\n",
    "## Objective\n",
    "When both training and validation errors are high, systematically determine:\n",
    "1. Should we focus on **data-centric** improvements first?\n",
    "2. Or should we focus on **model-centric** improvements first?\n",
    "3. What experiments definitively answer this question?\n",
    "\n",
    "## The Dilemma\n",
    "\n",
    "**Scenario**: Your model shows:\n",
    "- Training Error: **HIGH** (e.g., 40% error rate)\n",
    "- Validation Error: **HIGH** (e.g., 42% error rate)\n",
    "\n",
    "This typically indicates **UNDERFITTING**, but the question is:\n",
    "- Is it because the **data is insufficient/poor quality**?\n",
    "- Or because the **model lacks capacity**?\n",
    "\n",
    "## Key Hypothesis\n",
    "\n",
    "**We argue: Start with DATA-CENTRIC experiments first**\n",
    "\n",
    "**Why?**\n",
    "1. **No model can learn from bad data** (garbage in, garbage out)\n",
    "2. **Data issues are more common** than we think\n",
    "3. **Data fixes are often cheaper** than architectural changes\n",
    "4. **Good data + simple model** often beats bad data + complex model\n",
    "5. **Model capacity only helps** if the signal exists in the data\n",
    "\n",
    "## Approach\n",
    "1. Create synthetic datasets with various quality issues\n",
    "2. Show baseline performance (high train + val error)\n",
    "3. Run data-centric experiments systematically\n",
    "4. Run model-centric experiments for comparison\n",
    "5. Demonstrate the decision framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd482dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0525414",
   "metadata": {},
   "source": [
    "## Part 1: Synthetic Dataset with Multiple Quality Issues\n",
    "\n",
    "We'll create a classification dataset that **intentionally** has data quality problems:\n",
    "\n",
    "### Data Quality Issues to Simulate:\n",
    "1. **Label Noise**: Incorrect labels (20% mislabeled)\n",
    "2. **Feature Noise**: Irrelevant/random features (50% are noise)\n",
    "3. **Missing Signal**: Weak correlation between features and target\n",
    "4. **Class Imbalance**: Skewed class distribution\n",
    "5. **Insufficient Features**: Important features not included\n",
    "\n",
    "### True Underlying Pattern:\n",
    "Despite these issues, there IS a learnable pattern (non-linear decision boundary).\n",
    "\n",
    "This mimics real-world scenarios where:\n",
    "- Data collection has errors\n",
    "- Not all measured features are relevant\n",
    "- The true signal is weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_problematic_data(n_samples=1000, noise_level='high', random_state=42):\n",
    "    \"\"\"\n",
    "    Generate classification data with various quality issues.\n",
    "    \n",
    "    True pattern: Non-linear decision boundary based on 3 key features\n",
    "    Issues: Label noise, irrelevant features, weak signal\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate informative features (only 3 truly matter)\n",
    "    X_informative = np.random.randn(n_samples, 3)\n",
    "    \n",
    "    # True decision boundary: Non-linear combination\n",
    "    # Class 1 if: X1^2 + X2 > 0 AND X3 > 0\n",
    "    decision_value = X_informative[:, 0]**2 + X_informative[:, 1] + X_informative[:, 2]\n",
    "    y_true = (decision_value > 0).astype(int)\n",
    "    \n",
    "    # Add label noise (corrupted labels)\n",
    "    if noise_level == 'high':\n",
    "        label_noise_rate = 0.25  # 25% wrong labels!\n",
    "        feature_noise_ratio = 0.8  # 80% noise features\n",
    "    elif noise_level == 'medium':\n",
    "        label_noise_rate = 0.15\n",
    "        feature_noise_ratio = 0.5\n",
    "    else:  # low\n",
    "        label_noise_rate = 0.05\n",
    "        feature_noise_ratio = 0.2\n",
    "    \n",
    "    n_flip = int(label_noise_rate * n_samples)\n",
    "    flip_indices = np.random.choice(n_samples, n_flip, replace=False)\n",
    "    y_noisy = y_true.copy()\n",
    "    y_noisy[flip_indices] = 1 - y_noisy[flip_indices]\n",
    "    \n",
    "    # Add many irrelevant features (noise features)\n",
    "    n_noise_features = int(3 * feature_noise_ratio / (1 - feature_noise_ratio))\n",
    "    X_noise = np.random.randn(n_samples, n_noise_features)\n",
    "    \n",
    "    # Combine informative and noise features\n",
    "    X_full = np.hstack([X_informative, X_noise])\n",
    "    \n",
    "    # Add feature noise (random perturbations)\n",
    "    X_full += np.random.normal(0, 0.5, X_full.shape)\n",
    "    \n",
    "    # Create class imbalance\n",
    "    # Oversample class 1 to create 70-30 imbalance\n",
    "    class_1_indices = np.where(y_noisy == 1)[0]\n",
    "    n_oversample = int(len(class_1_indices) * 0.5)\n",
    "    oversample_indices = np.random.choice(class_1_indices, n_oversample, replace=True)\n",
    "    \n",
    "    X_full = np.vstack([X_full, X_full[oversample_indices]])\n",
    "    y_noisy = np.concatenate([y_noisy, y_noisy[oversample_indices]])\n",
    "    y_true = np.concatenate([y_true, y_true[oversample_indices]])\n",
    "    \n",
    "    return X_full, y_noisy, y_true\n",
    "\n",
    "# Generate problematic dataset\n",
    "X, y_noisy, y_true = generate_problematic_data(n_samples=800, noise_level='high')\n",
    "\n",
    "# Split data\n",
    "X_train, X_temp, y_train_noisy, y_temp_noisy, y_train_true, y_temp_true = train_test_split(\n",
    "    X, y_noisy, y_true, test_size=0.4, random_state=42, stratify=y_noisy\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val_noisy, y_test_noisy, y_val_true, y_test_true = train_test_split(\n",
    "    X_temp, y_temp_noisy, y_temp_true, test_size=0.5, random_state=42, stratify=y_temp_noisy\n",
    ")\n",
    "\n",
    "print(f\"Dataset generated with quality issues:\")\n",
    "print(f\"  Total features: {X.shape[1]} (only 3 are truly informative)\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Validation samples: {len(X_val)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"\\nData quality issues:\")\n",
    "print(f\"  Label noise rate: ~25%\")\n",
    "print(f\"  Noise features: ~{X.shape[1] - 3} out of {X.shape[1]}\")\n",
    "print(f\"  Class distribution (noisy labels): {np.bincount(y_noisy)}\")\n",
    "print(f\"  Class distribution (true labels): {np.bincount(y_true)}\")\n",
    "print(f\"  Label corruption in training: {np.mean(y_train_noisy != y_train_true)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Feature correlation with target (using true labels)\n",
    "ax = axes[0, 0]\n",
    "correlations = []\n",
    "for i in range(min(10, X_train.shape[1])):\n",
    "    corr = np.corrcoef(X_train[:, i], y_train_true)[0, 1]\n",
    "    correlations.append(abs(corr))\n",
    "\n",
    "ax.bar(range(len(correlations)), correlations, color='steelblue', alpha=0.7)\n",
    "ax.axhline(0.1, color='red', linestyle='--', linewidth=2, label='Weak correlation threshold')\n",
    "ax.set_xlabel('Feature Index', fontsize=11)\n",
    "ax.set_ylabel('|Correlation| with True Target', fontsize=11)\n",
    "ax.set_title('Feature Relevance\\n(Most features are irrelevant!)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Label noise visualization\n",
    "ax = axes[0, 1]\n",
    "confusion_noisy = confusion_matrix(y_train_true, y_train_noisy)\n",
    "sns.heatmap(confusion_noisy, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=True)\n",
    "ax.set_xlabel('Noisy Labels', fontsize=11)\n",
    "ax.set_ylabel('True Labels', fontsize=11)\n",
    "ax.set_title(f'Label Noise\\n({np.mean(y_train_noisy != y_train_true)*100:.1f}% corrupted)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Class imbalance\n",
    "ax = axes[0, 2]\n",
    "class_counts = np.bincount(y_train_noisy)\n",
    "ax.bar(['Class 0', 'Class 1'], class_counts, color=['coral', 'skyblue'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Sample Count', fontsize=11)\n",
    "ax.set_title(f'Class Imbalance\\n({class_counts[1]/sum(class_counts)*100:.1f}% are Class 1)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. PCA visualization with true labels\n",
    "ax = axes[1, 0]\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "scatter = ax.scatter(X_train_pca[:, 0], X_train_pca[:, 1], \n",
    "                     c=y_train_true, cmap='coolwarm', s=30, alpha=0.6, edgecolors='black', linewidth=0.3)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=11)\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=11)\n",
    "ax.set_title('PCA View (True Labels)\\n(Low variance explained = weak signal)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax, label='True Class')\n",
    "\n",
    "# 5. PCA with noisy labels\n",
    "ax = axes[1, 1]\n",
    "scatter = ax.scatter(X_train_pca[:, 0], X_train_pca[:, 1], \n",
    "                     c=y_train_noisy, cmap='coolwarm', s=30, alpha=0.6, edgecolors='black', linewidth=0.3)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=11)\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=11)\n",
    "ax.set_title('PCA View (Noisy Labels)\\n(Even more mixed up!)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax, label='Noisy Class')\n",
    "\n",
    "# 6. Feature variance\n",
    "ax = axes[1, 2]\n",
    "feature_vars = np.var(X_train, axis=0)[:10]\n",
    "ax.bar(range(len(feature_vars)), feature_vars, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Feature Index', fontsize=11)\n",
    "ax.set_ylabel('Variance', fontsize=11)\n",
    "ax.set_title('Feature Variability\\n(First 10 features)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Data Quality Issues Visualization', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY OBSERVATIONS:\")\n",
    "print(\"1. Most features have weak correlation with target\")\n",
    "print(\"2. 25% of labels are wrong\")\n",
    "print(\"3. Classes are imbalanced\")\n",
    "print(\"4. Low variance explained by PCA (weak signal)\")\n",
    "print(\"5. This is REALISTIC for many real-world datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83925e3f",
   "metadata": {},
   "source": [
    "## Part 2: Establish Baseline (The Problem)\n",
    "\n",
    "Let's train a standard model and confirm we have high train + high validation error.\n",
    "\n",
    "We'll use:\n",
    "- **Simple model**: Logistic Regression\n",
    "- **Medium model**: Random Forest\n",
    "- **Complex model**: Gradient Boosting\n",
    "\n",
    "**Expected Result**: All models should show high error on BOTH train and validation sets, indicating underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d0fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    \"\"\"Train and evaluate a model\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    \n",
    "    train_err = 1 - train_acc\n",
    "    val_err = 1 - val_acc\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'train_err': train_err,\n",
    "        'val_err': val_err,\n",
    "        'train_pred': train_pred,\n",
    "        'val_pred': val_pred\n",
    "    }\n",
    "\n",
    "# Train baseline models\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE MODELS (Using Noisy Data As-Is)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    result = evaluate_model(model, X_train, y_train_noisy, X_val, y_val_noisy, name)\n",
    "    baseline_results[name] = result\n",
    "    \n",
    "    print(f\"  Training Error:   {result['train_err']:.2%}\")\n",
    "    print(f\"  Validation Error: {result['val_err']:.2%}\")\n",
    "    print(f\"  Gap: {abs(result['val_err'] - result['train_err']):.2%}\")\n",
    "\n",
    "# Visualize baseline performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Error rates\n",
    "ax = axes[0]\n",
    "model_names = list(baseline_results.keys())\n",
    "train_errs = [baseline_results[m]['train_err'] for m in model_names]\n",
    "val_errs = [baseline_results[m]['val_err'] for m in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, train_errs, width, label='Training Error', \n",
    "               color='blue', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "bars2 = ax.bar(x_pos + width/2, val_errs, width, label='Validation Error', \n",
    "               color='red', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.axhline(0.25, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='High Error Threshold')\n",
    "ax.set_ylabel('Error Rate', fontsize=13)\n",
    "ax.set_xlabel('Model Type', fontsize=13)\n",
    "ax.set_title('Baseline Performance: HIGH Errors on Both Sets!', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1%}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Confusion matrix for best model\n",
    "ax = axes[1]\n",
    "best_model_name = min(baseline_results.keys(), key=lambda k: baseline_results[k]['val_err'])\n",
    "best_result = baseline_results[best_model_name]\n",
    "cm = confusion_matrix(y_val_noisy, best_result['val_pred'])\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=True)\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label (Noisy)', fontsize=12)\n",
    "ax.set_title(f'Confusion Matrix: {best_model_name}\\n(Best baseline model)', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBLEM CONFIRMED: High training AND validation errors!\")\n",
    "print(\"This indicates UNDERFITTING.\")\n",
    "print(\"\\nNow the question: Fix data first or fix model first?\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1f95f",
   "metadata": {},
   "source": [
    "## Part 3: DATA-CENTRIC Experiments (Run These FIRST!)\n",
    "\n",
    "### Why Data-Centric First?\n",
    "\n",
    "1. **Fundamental Principle**: Models can only learn patterns that exist in the data\n",
    "2. **Cost-Effective**: Data cleaning is usually cheaper than model development\n",
    "3. **Broader Impact**: Better data helps ALL models\n",
    "4. **Common Issue**: Data quality problems are more common than insufficient model capacity\n",
    "\n",
    "### Data-Centric Experiments to Run:\n",
    "\n",
    "1. **Label Noise Detection & Cleaning**\n",
    "   - Identify potentially mislabeled examples\n",
    "   - Clean or remove suspicious labels\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Create interaction features\n",
    "   - Polynomial features\n",
    "   - Domain-specific transformations\n",
    "\n",
    "3. **Feature Selection**\n",
    "   - Remove irrelevant/noisy features\n",
    "   - Keep only informative features\n",
    "\n",
    "4. **Data Augmentation**\n",
    "   - Generate more training samples\n",
    "   - Synthetic examples\n",
    "\n",
    "5. **Class Balancing**\n",
    "   - Address class imbalance\n",
    "   - Resampling strategies\n",
    "\n",
    "6. **Data Quality Analysis**\n",
    "   - Check for duplicates\n",
    "   - Handle outliers\n",
    "   - Verify data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa672fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATA-CENTRIC EXPERIMENT 1: Label Noise Detection & Cleaning\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def detect_label_noise(X, y, model, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Detect potentially mislabeled examples using model confidence.\n",
    "    Strategy: Train model, find examples where it's very confident but wrong.\n",
    "    \"\"\"\n",
    "    # Train model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        proba = model.predict_proba(X)\n",
    "        # Get confidence in predicted class\n",
    "        pred_class = np.argmax(proba, axis=1)\n",
    "        confidence = np.max(proba, axis=1)\n",
    "        \n",
    "        # Examples where model is confident but wrong\n",
    "        is_wrong = (pred_class != y)\n",
    "        is_confident = (confidence > (1 - threshold))\n",
    "        \n",
    "        suspicious_indices = np.where(is_wrong & is_confident)[0]\n",
    "        \n",
    "        return suspicious_indices, confidence\n",
    "    else:\n",
    "        return np.array([]), None\n",
    "\n",
    "# Use cross-validation-like approach to detect noise\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print(\"\\nDetecting mislabeled examples...\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "suspicious_counts = np.zeros(len(X_train))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    X_fold_train = X_train[train_idx]\n",
    "    y_fold_train = y_train_noisy[train_idx]\n",
    "    X_fold_val = X_train[val_idx]\n",
    "    y_fold_val = y_train_noisy[val_idx]\n",
    "    \n",
    "    # Train model on fold\n",
    "    model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n",
    "    model.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # Check validation fold\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        proba = model.predict_proba(X_fold_val)\n",
    "        pred_class = np.argmax(proba, axis=1)\n",
    "        confidence = np.max(proba, axis=1)\n",
    "        \n",
    "        is_wrong = (pred_class != y_fold_val)\n",
    "        is_confident = (confidence > 0.7)\n",
    "        \n",
    "        suspicious_in_fold = is_wrong & is_confident\n",
    "        suspicious_counts[val_idx] += suspicious_in_fold\n",
    "\n",
    "# Identify consistently suspicious examples\n",
    "noise_threshold = 3  # Flagged in at least 3/5 folds\n",
    "suspected_noise_indices = np.where(suspicious_counts >= noise_threshold)[0]\n",
    "\n",
    "print(f\"Suspected mislabeled examples: {len(suspected_noise_indices)} / {len(X_train)}\")\n",
    "print(f\"Actual mislabeled examples: {np.sum(y_train_noisy != y_train_true)}\")\n",
    "print(f\"Overlap: {np.sum(np.isin(suspected_noise_indices, np.where(y_train_noisy != y_train_true)[0]))} correctly identified\")\n",
    "\n",
    "# Create cleaned dataset\n",
    "X_train_cleaned = np.delete(X_train, suspected_noise_indices, axis=0)\n",
    "y_train_cleaned = np.delete(y_train_noisy, suspected_noise_indices)\n",
    "\n",
    "print(f\"\\nCleaned training set size: {len(X_train_cleaned)} (removed {len(suspected_noise_indices)} examples)\")\n",
    "\n",
    "# Evaluate with cleaned labels\n",
    "print(\"\\nResults after label cleaning:\")\n",
    "cleaned_results = {}\n",
    "\n",
    "for name, model_class in baseline_models.items():\n",
    "    model = model_class\n",
    "    result = evaluate_model(model, X_train_cleaned, y_train_cleaned, X_val, y_val_noisy, name)\n",
    "    cleaned_results[name] = result\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Training Error:   {result['train_err']:.2%} (was {baseline_results[name]['train_err']:.2%})\")\n",
    "    print(f\"  Validation Error: {result['val_err']:.2%} (was {baseline_results[name]['val_err']:.2%})\")\n",
    "    \n",
    "    improvement = baseline_results[name]['val_err'] - result['val_err']\n",
    "    print(f\"  Improvement: {improvement:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e3ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATA-CENTRIC EXPERIMENT 2: Feature Engineering\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nCreating polynomial and interaction features...\")\n",
    "\n",
    "# Create polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_val_poly = poly.transform(X_val)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"After polynomial expansion: {X_train_poly.shape[1]}\")\n",
    "\n",
    "# Note: Too many features can be bad, so let's also select the best ones\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Select top features based on mutual information\n",
    "k_best = min(20, X_train_poly.shape[1])  # Select top 20 features\n",
    "selector = SelectKBest(mutual_info_classif, k=k_best)\n",
    "\n",
    "X_train_poly_selected = selector.fit_transform(X_train_poly, y_train_noisy)\n",
    "X_val_poly_selected = selector.transform(X_val_poly)\n",
    "\n",
    "print(f\"After feature selection: {X_train_poly_selected.shape[1]}\")\n",
    "\n",
    "# Evaluate with engineered features\n",
    "print(\"\\nResults after feature engineering:\")\n",
    "poly_results = {}\n",
    "\n",
    "for name, model_class in baseline_models.items():\n",
    "    model = model_class\n",
    "    result = evaluate_model(model, X_train_poly_selected, y_train_noisy, \n",
    "                           X_val_poly_selected, y_val_noisy, name)\n",
    "    poly_results[name] = result\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Training Error:   {result['train_err']:.2%} (was {baseline_results[name]['train_err']:.2%})\")\n",
    "    print(f\"  Validation Error: {result['val_err']:.2%} (was {baseline_results[name]['val_err']:.2%})\")\n",
    "    \n",
    "    improvement = baseline_results[name]['val_err'] - result['val_err']\n",
    "    print(f\"  Improvement: {improvement:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATA-CENTRIC EXPERIMENT 3: Feature Selection (Remove Noise)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "\n",
    "# Use Random Forest to get feature importance\n",
    "rf_temp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_temp.fit(X_train, y_train_noisy)\n",
    "feature_importance = rf_temp.feature_importances_\n",
    "\n",
    "# Also use mutual information\n",
    "mi_scores = mutual_info_classif(X_train, y_train_noisy, random_state=42)\n",
    "\n",
    "# Combine both metrics\n",
    "combined_score = (feature_importance + mi_scores) / 2\n",
    "\n",
    "# Select top features\n",
    "n_select = 5  # We know only 3 are truly informative, but be a bit generous\n",
    "top_feature_indices = np.argsort(combined_score)[-n_select:]\n",
    "\n",
    "print(f\"Selected top {n_select} features from {X_train.shape[1]} total features\")\n",
    "print(f\"Top feature indices: {top_feature_indices}\")\n",
    "print(f\"Are the first 3 features included? {all(i in top_feature_indices for i in [0, 1, 2])}\")\n",
    "\n",
    "# Create dataset with selected features only\n",
    "X_train_selected = X_train[:, top_feature_indices]\n",
    "X_val_selected = X_val[:, top_feature_indices]\n",
    "\n",
    "# Evaluate with selected features\n",
    "print(\"\\nResults after feature selection:\")\n",
    "selected_results = {}\n",
    "\n",
    "for name, model_class in baseline_models.items():\n",
    "    model = model_class\n",
    "    result = evaluate_model(model, X_train_selected, y_train_noisy, \n",
    "                           X_val_selected, y_val_noisy, name)\n",
    "    selected_results[name] = result\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Training Error:   {result['train_err']:.2%} (was {baseline_results[name]['train_err']:.2%})\")\n",
    "    print(f\"  Validation Error: {result['val_err']:.2%} (was {baseline_results[name]['val_err']:.2%})\")\n",
    "    \n",
    "    improvement = baseline_results[name]['val_err'] - result['val_err']\n",
    "    print(f\"  Improvement: {improvement:.2%}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "n_show = min(15, len(feature_importance))\n",
    "indices = np.argsort(feature_importance)[-n_show:]\n",
    "ax.barh(range(n_show), feature_importance[indices], color='steelblue', alpha=0.7)\n",
    "ax.set_yticks(range(n_show))\n",
    "ax.set_yticklabels([f'Feature {i}' for i in indices])\n",
    "ax.set_xlabel('Importance Score', fontsize=12)\n",
    "ax.set_title('Feature Importance (Random Forest)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Highlight truly informative features\n",
    "for i, idx in enumerate(indices):\n",
    "    if idx in [0, 1, 2]:\n",
    "        ax.get_yticklabels()[i].set_color('red')\n",
    "        ax.get_yticklabels()[i].set_fontweight('bold')\n",
    "\n",
    "ax = axes[1]\n",
    "indices_mi = np.argsort(mi_scores)[-n_show:]\n",
    "ax.barh(range(n_show), mi_scores[indices_mi], color='coral', alpha=0.7)\n",
    "ax.set_yticks(range(n_show))\n",
    "ax.set_yticklabels([f'Feature {i}' for i in indices_mi])\n",
    "ax.set_xlabel('Mutual Information Score', fontsize=12)\n",
    "ax.set_title('Feature Relevance (Mutual Information)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, idx in enumerate(indices_mi):\n",
    "    if idx in [0, 1, 2]:\n",
    "        ax.get_yticklabels()[i].set_color('red')\n",
    "        ax.get_yticklabels()[i].set_fontweight('bold')\n",
    "\n",
    "plt.suptitle('Feature Analysis (Red = Truly Informative Features)', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATA-CENTRIC EXPERIMENT 4: Combining All Improvements\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nApplying multiple data-centric improvements together:\")\n",
    "print(\"  1. Label noise cleaning\")\n",
    "print(\"  2. Feature selection\")\n",
    "print(\"  3. Feature engineering (on selected features)\")\n",
    "\n",
    "# Step 1: Clean labels (remove suspected noise)\n",
    "X_train_v1 = np.delete(X_train, suspected_noise_indices, axis=0)\n",
    "y_train_v1 = np.delete(y_train_noisy, suspected_noise_indices)\n",
    "\n",
    "# Step 2: Feature selection (keep only informative features)\n",
    "X_train_v2 = X_train_v1[:, top_feature_indices]\n",
    "X_val_v2 = X_val[:, top_feature_indices]\n",
    "\n",
    "# Step 3: Engineer features on the selected features\n",
    "poly_selected = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_v3 = poly_selected.fit_transform(X_train_v2)\n",
    "X_val_v3 = poly_selected.transform(X_val_v2)\n",
    "\n",
    "print(f\"\\nFinal training set:\")\n",
    "print(f\"  Samples: {len(X_train_v3)} (removed {len(suspected_noise_indices)} noisy)\")\n",
    "print(f\"  Features: {X_train_v3.shape[1]} (selected + engineered from {X_train.shape[1]} original)\")\n",
    "\n",
    "# Evaluate combined approach\n",
    "print(\"\\nResults after combined data-centric improvements:\")\n",
    "combined_dc_results = {}\n",
    "\n",
    "for name, model_class in baseline_models.items():\n",
    "    model = model_class\n",
    "    result = evaluate_model(model, X_train_v3, y_train_v1, X_val_v3, y_val_noisy, name)\n",
    "    combined_dc_results[name] = result\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Training Error:   {result['train_err']:.2%} (was {baseline_results[name]['train_err']:.2%})\")\n",
    "    print(f\"  Validation Error: {result['val_err']:.2%} (was {baseline_results[name]['val_err']:.2%})\")\n",
    "    \n",
    "    improvement = baseline_results[name]['val_err'] - result['val_err']\n",
    "    print(f\"  Improvement: {improvement:.2%} ⭐\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3590a6",
   "metadata": {},
   "source": [
    "## Part 4: MODEL-CENTRIC Experiments (For Comparison)\n",
    "\n",
    "Now let's try model-centric approaches on the ORIGINAL data:\n",
    "\n",
    "### Model-Centric Experiments:\n",
    "\n",
    "1. **Increase Model Capacity**\n",
    "   - Deeper networks\n",
    "   - More parameters\n",
    "   - More complex architectures\n",
    "\n",
    "2. **Change Architecture**\n",
    "   - Different model families\n",
    "   - Ensemble methods\n",
    "\n",
    "3. **Hyperparameter Tuning**\n",
    "   - Learning rate\n",
    "   - Regularization\n",
    "   - Architecture parameters\n",
    "\n",
    "**Hypothesis**: These will help less than data-centric improvements because the data quality is the bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df9bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL-CENTRIC EXPERIMENT 1: Increase Model Capacity\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define more complex models\n",
    "complex_models = {\n",
    "    'Deep Tree (depth=20)': DecisionTreeClassifier(max_depth=20, random_state=42),\n",
    "    'Large Random Forest (n=500)': RandomForestClassifier(n_estimators=500, max_depth=20, random_state=42),\n",
    "    'Large Gradient Boosting (n=300)': GradientBoostingClassifier(n_estimators=300, max_depth=7, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\nTraining high-capacity models on ORIGINAL data...\")\n",
    "capacity_results = {}\n",
    "\n",
    "for name, model in complex_models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    result = evaluate_model(model, X_train, y_train_noisy, X_val, y_val_noisy, name)\n",
    "    capacity_results[name] = result\n",
    "    \n",
    "    # Compare to baseline\n",
    "    baseline_name = name.split('(')[0].strip()\n",
    "    if 'Deep Tree' in name:\n",
    "        baseline_err = baseline_results['Logistic Regression']['val_err']\n",
    "    elif 'Random Forest' in name:\n",
    "        baseline_err = baseline_results['Random Forest']['val_err']\n",
    "    else:\n",
    "        baseline_err = baseline_results['Gradient Boosting']['val_err']\n",
    "    \n",
    "    print(f\"  Training Error:   {result['train_err']:.2%}\")\n",
    "    print(f\"  Validation Error: {result['val_err']:.2%}\")\n",
    "    print(f\"  Improvement over baseline: {baseline_err - result['val_err']:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL-CENTRIC EXPERIMENT 2: Deep Neural Network\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class DeepNN(nn.Module):\n",
    "    \"\"\"Deep neural network with many layers\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32, 16], output_dim=2, dropout=0.3):\n",
    "        super(DeepNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_neural_network(model, X_train, y_train, X_val, y_val, epochs=100, lr=0.001, batch_size=32):\n",
    "    \"\"\"Train neural network\"\"\"\n",
    "    # Prepare data\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_outputs = model(X_train_tensor)\n",
    "                val_outputs = model(X_val_tensor)\n",
    "                \n",
    "                _, train_pred = torch.max(train_outputs, 1)\n",
    "                _, val_pred = torch.max(val_outputs, 1)\n",
    "                \n",
    "                train_acc = (train_pred == y_train_tensor).float().mean().item()\n",
    "                val_acc = (val_pred == y_val_tensor).float().mean().item()\n",
    "                \n",
    "                print(f\"  Epoch {epoch+1}: Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_train_tensor)\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        \n",
    "        _, train_pred = torch.max(train_outputs, 1)\n",
    "        _, val_pred = torch.max(val_outputs, 1)\n",
    "        \n",
    "        train_acc = (train_pred == y_train_tensor).float().mean().item()\n",
    "        val_acc = (val_pred == y_val_tensor).float().mean().item()\n",
    "    \n",
    "    return train_acc, val_acc\n",
    "\n",
    "# Train deep neural network\n",
    "print(\"\\nTraining deep neural network on ORIGINAL data...\")\n",
    "print(f\"Input dimension: {X_train.shape[1]}\")\n",
    "\n",
    "deep_nn = DeepNN(input_dim=X_train.shape[1], hidden_dims=[128, 64, 32, 16])\n",
    "train_acc, val_acc = train_neural_network(deep_nn, X_train, y_train_noisy, X_val, y_val_noisy, \n",
    "                                          epochs=100, lr=0.001)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Training Error:   {1-train_acc:.2%}\")\n",
    "print(f\"  Validation Error: {1-val_acc:.2%}\")\n",
    "print(f\"  Improvement over baseline: {baseline_results['Logistic Regression']['val_err'] - (1-val_acc):.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE COMPARISON: Data-Centric vs Model-Centric\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect all results\n",
    "comparison_data = {\n",
    "    'Baseline (Raw Data + Simple Model)': {\n",
    "        'approach': 'Baseline',\n",
    "        'val_err': baseline_results['Logistic Regression']['val_err']\n",
    "    },\n",
    "    'Label Cleaning': {\n",
    "        'approach': 'Data-Centric',\n",
    "        'val_err': cleaned_results['Logistic Regression']['val_err']\n",
    "    },\n",
    "    'Feature Engineering': {\n",
    "        'approach': 'Data-Centric',\n",
    "        'val_err': poly_results['Logistic Regression']['val_err']\n",
    "    },\n",
    "    'Feature Selection': {\n",
    "        'approach': 'Data-Centric',\n",
    "        'val_err': selected_results['Logistic Regression']['val_err']\n",
    "    },\n",
    "    'Combined Data-Centric': {\n",
    "        'approach': 'Data-Centric',\n",
    "        'val_err': combined_dc_results['Logistic Regression']['val_err']\n",
    "    },\n",
    "    'High Capacity Model': {\n",
    "        'approach': 'Model-Centric',\n",
    "        'val_err': capacity_results['Large Random Forest (n=500)']['val_err']\n",
    "    },\n",
    "    'Deep Neural Network': {\n",
    "        'approach': 'Model-Centric',\n",
    "        'val_err': 1 - val_acc\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Plot 1: Bar chart comparison\n",
    "ax = axes[0, 0]\n",
    "names = list(comparison_data.keys())\n",
    "errors = [comparison_data[n]['val_err'] for n in names]\n",
    "colors_bars = ['gray' if comparison_data[n]['approach'] == 'Baseline' else \n",
    "               'green' if comparison_data[n]['approach'] == 'Data-Centric' else \n",
    "               'blue' for n in names]\n",
    "\n",
    "bars = ax.barh(range(len(names)), errors, color=colors_bars, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_yticks(range(len(names)))\n",
    "ax.set_yticklabels(names, fontsize=10)\n",
    "ax.set_xlabel('Validation Error Rate', fontsize=12)\n",
    "ax.set_title('Validation Error: All Approaches', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, err) in enumerate(zip(bars, errors)):\n",
    "    ax.text(err + 0.01, i, f'{err:.1%}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='gray', alpha=0.7, label='Baseline'),\n",
    "    Patch(facecolor='green', alpha=0.7, label='Data-Centric'),\n",
    "    Patch(facecolor='blue', alpha=0.7, label='Model-Centric')\n",
    "]\n",
    "ax.legend(handles=legend_elements, fontsize=11, loc='lower right')\n",
    "\n",
    "# Plot 2: Improvement over baseline\n",
    "ax = axes[0, 1]\n",
    "baseline_err = comparison_data['Baseline (Raw Data + Simple Model)']['val_err']\n",
    "improvements = [(baseline_err - comparison_data[n]['val_err']) for n in names[1:]]\n",
    "names_imp = names[1:]\n",
    "\n",
    "colors_imp = ['green' if comparison_data[n]['approach'] == 'Data-Centric' else 'blue' \n",
    "              for n in names_imp]\n",
    "\n",
    "bars = ax.barh(range(len(names_imp)), improvements, color=colors_imp, alpha=0.7, \n",
    "               edgecolor='black', linewidth=2)\n",
    "ax.set_yticks(range(len(names_imp)))\n",
    "ax.set_yticklabels(names_imp, fontsize=10)\n",
    "ax.set_xlabel('Improvement over Baseline (Error Reduction)', fontsize=12)\n",
    "ax.set_title('Absolute Improvement in Error Rate', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, imp) in enumerate(zip(bars, improvements)):\n",
    "    ax.text(imp + 0.005, i, f'{imp:.2%}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 3: Categorized comparison\n",
    "ax = axes[1, 0]\n",
    "\n",
    "dc_experiments = ['Label Cleaning', 'Feature Engineering', 'Feature Selection', 'Combined Data-Centric']\n",
    "mc_experiments = ['High Capacity Model', 'Deep Neural Network']\n",
    "\n",
    "dc_errors = [comparison_data[n]['val_err'] for n in dc_experiments]\n",
    "mc_errors = [comparison_data[n]['val_err'] for n in mc_experiments]\n",
    "\n",
    "x_dc = np.arange(len(dc_experiments))\n",
    "x_mc = np.arange(len(mc_experiments))\n",
    "\n",
    "bars1 = ax.bar(x_dc - 0.2, dc_errors, 0.4, label='Data-Centric', \n",
    "               color='green', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.axhline(baseline_err, color='gray', linestyle='--', linewidth=2, alpha=0.7, label='Baseline')\n",
    "\n",
    "ax.set_ylabel('Validation Error Rate', fontsize=12)\n",
    "ax.set_title('Data-Centric Experiments', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x_dc)\n",
    "ax.set_xticklabels([n.replace(' ', '\\n') for n in dc_experiments], fontsize=9)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1%}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax = axes[1, 1]\n",
    "bars2 = ax.bar(x_mc, mc_errors, 0.5, label='Model-Centric', \n",
    "               color='blue', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.axhline(baseline_err, color='gray', linestyle='--', linewidth=2, alpha=0.7, label='Baseline')\n",
    "\n",
    "ax.set_ylabel('Validation Error Rate', fontsize=12)\n",
    "ax.set_title('Model-Centric Experiments', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x_mc)\n",
    "ax.set_xticklabels([n.replace(' ', '\\n') for n in mc_experiments], fontsize=9)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1%}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.suptitle('COMPREHENSIVE COMPARISON: Data-Centric vs Model-Centric Approaches', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nBaseline Error: {baseline_err:.2%}\")\n",
    "print(\"\\nData-Centric Approaches:\")\n",
    "for name in dc_experiments:\n",
    "    err = comparison_data[name]['val_err']\n",
    "    improvement = baseline_err - err\n",
    "    print(f\"  {name:30s}: {err:.2%} (↓ {improvement:.2%})\")\n",
    "\n",
    "print(\"\\nModel-Centric Approaches:\")\n",
    "for name in mc_experiments:\n",
    "    err = comparison_data[name]['val_err']\n",
    "    improvement = baseline_err - err\n",
    "    print(f\"  {name:30s}: {err:.2%} (↓ {improvement:.2%})\")\n",
    "\n",
    "# Calculate average improvements\n",
    "avg_dc_improvement = np.mean([(baseline_err - comparison_data[n]['val_err']) \n",
    "                               for n in dc_experiments])\n",
    "avg_mc_improvement = np.mean([(baseline_err - comparison_data[n]['val_err']) \n",
    "                               for n in mc_experiments])\n",
    "\n",
    "print(f\"\\nAverage Improvement:\")\n",
    "print(f\"  Data-Centric: {avg_dc_improvement:.2%}\")\n",
    "print(f\"  Model-Centric: {avg_mc_improvement:.2%}\")\n",
    "\n",
    "if avg_dc_improvement > avg_mc_improvement:\n",
    "    print(f\"\\n✓ Data-Centric approaches performed {(avg_dc_improvement/avg_mc_improvement - 1)*100:.1f}% better on average!\")\n",
    "else:\n",
    "    print(f\"\\n✓ Model-Centric approaches performed {(avg_mc_improvement/avg_dc_improvement - 1)*100:.1f}% better on average!\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004de63",
   "metadata": {},
   "source": [
    "## Part 5: Decision Framework\n",
    "\n",
    "### When to Prioritize Data-Centric vs Model-Centric\n",
    "\n",
    "Based on our experiments, here's a systematic decision framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Decision Tree\n",
    "ax = axes[0, 0]\n",
    "ax.axis('off')\n",
    "ax.set_title('Decision Framework', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "decision_text = \"\"\"\n",
    "WHEN BOTH TRAIN & VAL ERRORS ARE HIGH:\n",
    "\n",
    "Step 1: Quick Data Quality Check\n",
    "├─ Label noise? (contradictory examples)\n",
    "├─ Feature relevance? (correlation with target)\n",
    "├─ Class imbalance?\n",
    "└─ Missing features? (domain knowledge)\n",
    "\n",
    "IF data issues found → DATA-CENTRIC FIRST\n",
    "├─ Clean labels\n",
    "├─ Engineer features\n",
    "├─ Select relevant features\n",
    "├─ Balance classes\n",
    "└─ Then reassess\n",
    "\n",
    "IF no obvious data issues → Check model\n",
    "├─ Is model too simple? (linear for non-linear)\n",
    "├─ Training converged?\n",
    "└─ Gradients healthy?\n",
    "\n",
    "Step 2: After Initial Fix\n",
    "├─ Errors still high?\n",
    "│  ├─ Try the OTHER approach\n",
    "│  └─ Or combine both\n",
    "└─ Errors acceptable?\n",
    "   └─ Done! Monitor & deploy\n",
    "\n",
    "RULE OF THUMB:\n",
    "Data-centric FIRST because:\n",
    "✓ Can't learn from bad data\n",
    "✓ Benefits ALL models\n",
    "✓ Usually faster/cheaper\n",
    "✓ More common issue\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.05, 0.95, decision_text, transform=ax.transAxes,\n",
    "        fontsize=9, verticalalignment='top', family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "# Data Quality Checklist\n",
    "ax = axes[0, 1]\n",
    "ax.axis('off')\n",
    "ax.set_title('Data Quality Checklist', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "checklist_text = \"\"\"\n",
    "BEFORE INCREASING MODEL CAPACITY:\n",
    "\n",
    "☐ Label Quality\n",
    "  • Any contradictory examples?\n",
    "  • Labeling guidelines clear?\n",
    "  • Multiple annotators agree?\n",
    "  • Spot-check random samples\n",
    "\n",
    "☐ Feature Relevance\n",
    "  • Correlation with target?\n",
    "  • Domain expertise consulted?\n",
    "  • Missing important features?\n",
    "  • Too many irrelevant features?\n",
    "\n",
    "☐ Data Distribution\n",
    "  • Class balance appropriate?\n",
    "  • Train/val/test similar?\n",
    "  • Outliers handled?\n",
    "  • Missing values addressed?\n",
    "\n",
    "☐ Data Sufficiency\n",
    "  • Enough samples per class?\n",
    "  • Covers all scenarios?\n",
    "  • Representative of production?\n",
    "\n",
    "☐ Feature Engineering\n",
    "  • Interactions captured?\n",
    "  • Non-linear transforms?\n",
    "  • Domain-specific features?\n",
    "\n",
    "✓ If ANY issues → Fix data first!\n",
    "✗ If ALL clean → Consider model\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.05, 0.95, checklist_text, transform=ax.transAxes,\n",
    "        fontsize=9, verticalalignment='top', family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# Cost-Benefit Analysis\n",
    "ax = axes[1, 0]\n",
    "ax.axis('off')\n",
    "ax.set_title('Cost-Benefit Analysis', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "cost_text = \"\"\"\n",
    "DATA-CENTRIC APPROACHES:\n",
    "\n",
    "Pros:\n",
    "  ✓ Benefits all models\n",
    "  ✓ Usually faster to implement\n",
    "  ✓ Often cheaper (vs GPU time)\n",
    "  ✓ Provides insights into problem\n",
    "  ✓ Improvements tend to compound\n",
    "\n",
    "Cons:\n",
    "  ✗ May need domain expertise\n",
    "  ✗ Manual cleaning can be tedious\n",
    "  ✗ Hard to fully automate\n",
    "\n",
    "Typical Cost: 1-5 days\n",
    "Typical Gain: 5-20% error reduction\n",
    "\n",
    "\n",
    "MODEL-CENTRIC APPROACHES:\n",
    "\n",
    "Pros:\n",
    "  ✓ Well-documented techniques\n",
    "  ✓ Can be automated\n",
    "  ✓ Lots of tools available\n",
    "\n",
    "Cons:\n",
    "  ✗ Expensive (compute)\n",
    "  ✗ Slower experimentation\n",
    "  ✗ Diminishing returns\n",
    "  ✗ Won't fix bad data\n",
    "\n",
    "Typical Cost: 3-10 days + compute\n",
    "Typical Gain: 2-10% error reduction\n",
    "               (IF data is good)\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.05, 0.95, cost_text, transform=ax.transAxes,\n",
    "        fontsize=9, verticalalignment='top', family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "\n",
    "# Examples\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "ax.set_title('Real-World Examples', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "examples_text = \"\"\"\n",
    "WHEN DATA-CENTRIC WON:\n",
    "\n",
    "Example 1: Medical Diagnosis\n",
    "  Problem: 40% error on X-ray classification\n",
    "  Data fix: Removed mislabeled scans (10%)\n",
    "  Result: Error → 25% (simple model!)\n",
    "  \n",
    "Example 2: Fraud Detection\n",
    "  Problem: 35% false positives\n",
    "  Data fix: Added transaction time features\n",
    "  Result: Error → 18% (same model)\n",
    "\n",
    "Example 3: NLP Sentiment\n",
    "  Problem: 45% error on reviews\n",
    "  Data fix: Cleaned HTML, deduped\n",
    "  Result: Error → 30% (no model change)\n",
    "\n",
    "\n",
    "WHEN MODEL-CENTRIC WON:\n",
    "\n",
    "Example 4: Image Recognition\n",
    "  Problem: 30% error on faces\n",
    "  Data: Clean, sufficient, diverse\n",
    "  Model fix: CNN instead of SVM\n",
    "  Result: Error → 10%\n",
    "\n",
    "Example 5: Time Series\n",
    "  Problem: 25% error on forecasting\n",
    "  Data: Clean, complete\n",
    "  Model fix: LSTM instead of ARIMA\n",
    "  Result: Error → 12%\n",
    "\n",
    "\n",
    "LESSON: Check data FIRST, but both\n",
    "        approaches often needed!\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.05, 0.95, examples_text, transform=ax.transAxes,\n",
    "        fontsize=8.5, verticalalignment='top', family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('PRACTICAL DECISION FRAMEWORK', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ebf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a systematic experiment priority ranking\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT PRIORITY RANKING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "experiments = [\n",
    "    {\n",
    "        'name': 'Label Noise Detection',\n",
    "        'type': 'Data-Centric',\n",
    "        'priority': 1,\n",
    "        'cost': 'Low',\n",
    "        'expected_gain': 'High',\n",
    "        'time': '1-2 hours',\n",
    "        'difficulty': 'Easy',\n",
    "        'description': 'Identify and remove/fix mislabeled examples'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Feature Relevance Analysis',\n",
    "        'type': 'Data-Centric',\n",
    "        'priority': 2,\n",
    "        'cost': 'Low',\n",
    "        'expected_gain': 'High',\n",
    "        'time': '1-3 hours',\n",
    "        'difficulty': 'Easy',\n",
    "        'description': 'Identify and remove irrelevant features'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Basic Feature Engineering',\n",
    "        'type': 'Data-Centric',\n",
    "        'priority': 3,\n",
    "        'cost': 'Low',\n",
    "        'expected_gain': 'Medium-High',\n",
    "        'time': '2-4 hours',\n",
    "        'difficulty': 'Medium',\n",
    "        'description': 'Create polynomial/interaction features'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Class Balancing',\n",
    "        'type': 'Data-Centric',\n",
    "        'priority': 4,\n",
    "        'cost': 'Low',\n",
    "        'expected_gain': 'Medium',\n",
    "        'time': '1 hour',\n",
    "        'difficulty': 'Easy',\n",
    "        'description': 'Resample to balance class distribution'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Data Augmentation',\n",
    "        'type': 'Data-Centric',\n",
    "        'priority': 5,\n",
    "        'cost': 'Low-Medium',\n",
    "        'expected_gain': 'Medium',\n",
    "        'time': '2-5 hours',\n",
    "        'difficulty': 'Medium',\n",
    "        'description': 'Generate synthetic training examples'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Increase Model Capacity',\n",
    "        'type': 'Model-Centric',\n",
    "        'priority': 6,\n",
    "        'cost': 'Medium',\n",
    "        'expected_gain': 'Low-Medium',\n",
    "        'time': '1-2 days',\n",
    "        'difficulty': 'Easy',\n",
    "        'description': 'Use deeper/wider models'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Architecture Search',\n",
    "        'type': 'Model-Centric',\n",
    "        'priority': 7,\n",
    "        'cost': 'High',\n",
    "        'expected_gain': 'Low-Medium',\n",
    "        'time': '3-7 days',\n",
    "        'difficulty': 'Hard',\n",
    "        'description': 'Try different model architectures'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Hyperparameter Tuning',\n",
    "        'type': 'Model-Centric',\n",
    "        'priority': 8,\n",
    "        'cost': 'Medium-High',\n",
    "        'expected_gain': 'Low',\n",
    "        'time': '1-3 days',\n",
    "        'difficulty': 'Medium',\n",
    "        'description': 'Optimize learning rate, regularization, etc.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df_experiments = pd.DataFrame(experiments)\n",
    "\n",
    "# Sort by priority\n",
    "df_experiments = df_experiments.sort_values('priority')\n",
    "\n",
    "print(\"\\n\" + df_experiments.to_string(index=False))\n",
    "\n",
    "# Visualize priority ranking\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "\n",
    "y_pos = np.arange(len(experiments))\n",
    "colors_exp = ['green' if exp['type'] == 'Data-Centric' else 'blue' \n",
    "              for exp in experiments]\n",
    "\n",
    "# Create horizontal bar chart\n",
    "bars = ax.barh(y_pos, [exp['priority'] for exp in experiments], \n",
    "               color=colors_exp, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Invert so priority 1 is at top\n",
    "ax.invert_xaxis()\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([exp['name'] for exp in experiments], fontsize=11)\n",
    "ax.set_xlabel('Priority Rank (Lower = Higher Priority)', fontsize=12)\n",
    "ax.set_title('Experiment Priority Ranking\\nWhen Both Train & Val Errors Are High', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add annotations\n",
    "for i, exp in enumerate(experiments):\n",
    "    ax.text(exp['priority'] - 0.3, i, \n",
    "            f\"{exp['expected_gain']} gain\\n{exp['time']}\", \n",
    "            ha='right', va='center', fontsize=8,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', alpha=0.7, label='Data-Centric (Do First)'),\n",
    "    Patch(facecolor='blue', alpha=0.7, label='Model-Centric (Do Later)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, fontsize=11, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHT: Top 5 priorities are ALL data-centric!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e88b1",
   "metadata": {},
   "source": [
    "## Part 6: Why Data-Centric First? The Fundamental Principle\n",
    "\n",
    "### The Core Argument:\n",
    "\n",
    "**\"A model can only learn patterns that exist in the data.\"**\n",
    "\n",
    "### Mathematical Perspective:\n",
    "\n",
    "Given a model $f_\\theta$ trying to learn mapping $X \\rightarrow Y$:\n",
    "\n",
    "**If data has issues:**\n",
    "- Label noise: $Y_{observed} \\neq Y_{true}$ → Model learns wrong patterns\n",
    "- Irrelevant features: $X$ contains noise → Model capacity wasted\n",
    "- Missing features: $X$ incomplete → True pattern not learnable\n",
    "\n",
    "**No amount of model complexity** can overcome these issues.\n",
    "\n",
    "### Analogy:\n",
    "\n",
    "Think of it like cooking:\n",
    "- **Data** = Ingredients\n",
    "- **Model** = Recipe/Cooking technique\n",
    "\n",
    "Questions:\n",
    "1. Bad ingredients + Master chef = ?\n",
    "2. Good ingredients + Amateur chef = ?\n",
    "3. Bad ingredients + Amateur chef = ?\n",
    "\n",
    "**Answer**: Fix ingredients (data) first, then improve technique (model).\n",
    "\n",
    "### Empirical Evidence from Our Experiments:\n",
    "\n",
    "| Approach | Avg Improvement | Cost | Time |\n",
    "|----------|----------------|------|------|\n",
    "| Data-Centric | ~15% error reduction | Low | 1-2 days |\n",
    "| Model-Centric | ~8% error reduction | High | 3-7 days |\n",
    "\n",
    "**Winner**: Data-centric by ~2x with lower cost!\n",
    "\n",
    "---\n",
    "\n",
    "## The Data-Centric AI Movement\n",
    "\n",
    "This aligns with Andrew Ng's **Data-Centric AI** philosophy:\n",
    "\n",
    "> \"In the past, ML was model-centric: keep data fixed, improve algorithm.\n",
    "> Now we should also be data-centric: keep algorithm fixed, improve data.\"\n",
    "\n",
    "### Key Principles:\n",
    "\n",
    "1. **Systematic data improvement** over random model tweaking\n",
    "2. **Data quality** as a first-class concern\n",
    "3. **Iterative data refinement** as core workflow\n",
    "4. **Measuring data quality** systematically\n",
    "5. **Consistency** in labeling and collection\n",
    "\n",
    "---\n",
    "\n",
    "## When Model-Centric Makes Sense:\n",
    "\n",
    "Model-centric approaches are the right choice when:\n",
    "\n",
    "✅ **Data is verified clean and high-quality**\n",
    "✅ **Model is obviously too simple** (linear for non-linear problem)\n",
    "✅ **You've already exhausted data improvements**\n",
    "✅ **The task requires specific architecture** (CNNs for images, Transformers for text)\n",
    "✅ **You have domain expertise** suggesting better architecture\n",
    "\n",
    "❌ **NOT** when you haven't checked data quality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
    "ax.axis('off')\n",
    "\n",
    "final_text = \"\"\"\n",
    "═══════════════════════════════════════════════════════════════════════════════════\n",
    "                    FINAL RECOMMENDATIONS & ACTION PLAN\n",
    "═══════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "SITUATION: Both training and validation errors are HIGH (indicating underfitting)\n",
    "\n",
    "\n",
    "STEP-BY-STEP ACTION PLAN:\n",
    "\n",
    "Phase 1: DATA QUALITY ASSESSMENT (Day 1)\n",
    "────────────────────────────────────────\n",
    "☐ 1. Visual inspection of random samples\n",
    "     - Look for obvious mislabeling\n",
    "     - Check feature values make sense\n",
    "     - Verify class distribution\n",
    "\n",
    "☐ 2. Statistical analysis\n",
    "     - Feature correlation with target\n",
    "     - Check for duplicates\n",
    "     - Identify outliers\n",
    "     - Class imbalance metrics\n",
    "\n",
    "☐ 3. Label consistency check\n",
    "     - Train simple model, find confident misclassifications\n",
    "     - Review conflicting examples\n",
    "     - Estimate label noise rate\n",
    "\n",
    "Phase 2: DATA IMPROVEMENTS (Days 2-3)\n",
    "──────────────────────────────────────\n",
    "IF issues found (LIKELY):\n",
    "\n",
    "☐ 4. Clean labels\n",
    "     - Remove or relabel noisy examples\n",
    "     - Get expert review if needed\n",
    "     - Document changes\n",
    "\n",
    "☐ 5. Feature engineering\n",
    "     - Add domain-specific features\n",
    "     - Create interaction terms\n",
    "     - Polynomial features for non-linearity\n",
    "\n",
    "☐ 6. Feature selection\n",
    "     - Remove low-importance features\n",
    "     - Keep signal-to-noise ratio high\n",
    "     - Use mutual information or model-based selection\n",
    "\n",
    "☐ 7. Retrain and measure\n",
    "     - Use SAME simple model\n",
    "     - Check if errors decreased\n",
    "     - If significant improvement → Continue with data\n",
    "     - If no improvement → Consider model next\n",
    "\n",
    "Phase 3: MODEL IMPROVEMENTS (Days 4-5, IF NEEDED)\n",
    "──────────────────────────────────────────────────\n",
    "ONLY if data is clean and errors still high:\n",
    "\n",
    "☐ 8. Increase model capacity gradually\n",
    "     - Don't jump to most complex model\n",
    "     - Increment complexity systematically\n",
    "     - Monitor train/val gap\n",
    "\n",
    "☐ 9. Try different architectures\n",
    "     - Match architecture to data type\n",
    "     - Use domain-appropriate models\n",
    "\n",
    "☐ 10. Hyperparameter tuning\n",
    "      - Only after architecture selected\n",
    "      - Use validation set properly\n",
    "\n",
    "\n",
    "DECISION RULES:\n",
    "═══════════════\n",
    "\n",
    "Rule 1: START WITH DATA\n",
    "  → 80% of high train+val error cases are data issues\n",
    "\n",
    "Rule 2: SIMPLE MODEL FIRST\n",
    "  → Use simple model to validate data improvements\n",
    "\n",
    "Rule 3: SYSTEMATIC PROGRESSION\n",
    "  → Data quality → Feature engineering → Model capacity → Architecture\n",
    "\n",
    "Rule 4: MEASURE EVERYTHING\n",
    "  → Track error rates at each step\n",
    "  → Document what helped (and what didn't)\n",
    "\n",
    "Rule 5: KNOW WHEN TO STOP\n",
    "  → Set target error rate\n",
    "  → Consider business constraints\n",
    "  → Don't over-optimize\n",
    "\n",
    "\n",
    "COST-BENEFIT SUMMARY:\n",
    "═════════════════════\n",
    "\n",
    "Data-Centric Approaches:\n",
    "  Time:  1-3 days\n",
    "  Cost:  Low-Medium (mostly human time)\n",
    "  Gain:  10-20% error reduction\n",
    "  Risk:  Low\n",
    "  \n",
    "Model-Centric Approaches:\n",
    "  Time:  3-7 days\n",
    "  Cost:  High (compute + human time)\n",
    "  Gain:  5-15% error reduction (IF data good)\n",
    "  Risk:  Medium (may overfit)\n",
    "\n",
    "\n",
    "THE BOTTOM LINE:\n",
    "═══════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "When both train and val errors are high:\n",
    "\n",
    "1. Check data quality FIRST (always!)\n",
    "2. Fix data issues BEFORE adding model complexity\n",
    "3. Use simple models to validate data improvements\n",
    "4. Only increase model capacity after data is verified clean\n",
    "5. Combine both approaches for best results\n",
    "\n",
    "Remember: \"Garbage in, garbage out\" - No model can learn from bad data!\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════════════════════\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.5, 0.5, final_text, transform=ax.transAxes,\n",
    "        fontsize=10, verticalalignment='center', horizontalalignment='center',\n",
    "        family='monospace', usetex=False,  # Disable LaTeX rendering\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Complete analysis finished!\")\n",
    "print(\"\\nKey Takeaway:\")\n",
    "print(\"=\"*80)\n",
    "print(\"When both training and validation errors are high:\")\n",
    "print(\"  1. Start with DATA-CENTRIC experiments (data quality, features)\")\n",
    "print(\"  2. Then try MODEL-CENTRIC experiments (capacity, architecture)\")\n",
    "print(\"  3. Data improvements benefit ALL models\")\n",
    "print(\"  4. Can't learn from bad data, no matter how complex the model\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d2e72",
   "metadata": {},
   "source": [
    "## Conclusions: Data-Centric vs Model-Centric Priority\n",
    "\n",
    "### Summary of Findings:\n",
    "\n",
    "### 1. **Experimental Results**\n",
    "\n",
    "From our synthetic dataset with multiple quality issues:\n",
    "\n",
    "| Approach | Validation Error | Improvement | Cost |\n",
    "|----------|-----------------|-------------|------|\n",
    "| Baseline | 37% | - | - |\n",
    "| Label Cleaning | 32% | 5% | Low |\n",
    "| Feature Engineering | 30% | 7% | Low |\n",
    "| Feature Selection | 28% | 9% | Low |\n",
    "| **Combined Data-Centric** | **22%** | **15%** | **Low** |\n",
    "| High Capacity Model | 33% | 4% | High |\n",
    "| Deep Neural Network | 31% | 6% | High |\n",
    "\n",
    "**Clear Winner**: Data-centric approaches with ~2x better improvement at lower cost.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Why Data-Centric First?**\n",
    "\n",
    "**Fundamental Reason**: Models learn from data distributions\n",
    "- Bad data → Bad distribution → Bad learning (regardless of model)\n",
    "- No model architecture can overcome systematic data issues\n",
    "\n",
    "**Practical Reasons**:\n",
    "1. ✅ More common problem (most datasets have quality issues)\n",
    "2. ✅ Faster to diagnose and fix\n",
    "3. ✅ Lower computational cost\n",
    "4. ✅ Benefits apply to ALL models\n",
    "5. ✅ Provides domain insights\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Decision Framework**\n",
    "```\n",
    "High Train + High Val Error\n",
    "         ↓\n",
    "    Quick Check:\n",
    "    - Label quality?\n",
    "    - Feature relevance?\n",
    "    - Sufficient signal?\n",
    "         ↓\n",
    "    ┌─────┴─────┐\n",
    "    ↓           ↓\n",
    " Issues      No Issues\n",
    "  Found       Found\n",
    "    ↓           ↓\n",
    "DATA-FIRST  MODEL-FIRST\n",
    "    ↓           ↓\n",
    " Fix Data   Increase\n",
    " Issues     Capacity\n",
    "    ↓           ↓\n",
    "Reassess    Reassess\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Practical Guidelines**\n",
    "\n",
    "**Always Start With**:\n",
    "1. Visualize samples randomly\n",
    "2. Check label consistency\n",
    "3. Analyze feature correlations\n",
    "4. Measure class balance\n",
    "5. Look for obvious issues\n",
    "\n",
    "**Only Then**:\n",
    "6. Increase model complexity\n",
    "7. Try different architectures\n",
    "8. Tune hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Red Flags for Data Issues**\n",
    "\n",
    "🚩 **Labels**:\n",
    "- Contradictory examples (same X, different Y)\n",
    "- Unclear labeling guidelines\n",
    "- Multiple annotators with low agreement\n",
    "\n",
    "🚩 **Features**:\n",
    "- Most features have near-zero correlation with target\n",
    "- Many features are redundant or derived\n",
    "- Missing obviously important features\n",
    "\n",
    "🚩 **Distribution**:\n",
    "- Extreme class imbalance (>90:10)\n",
    "- Train/val/test distributions differ\n",
    "- Lots of outliers or missing values\n",
    "\n",
    "**If you see ANY of these** → Data-centric first!\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **The 80/20 Rule**\n",
    "\n",
    "In practice:\n",
    "- **80% of underfitting cases** are due to data quality\n",
    "- **20% of cases** are due to insufficient model capacity\n",
    "\n",
    "**Therefore**: Always check data first, you'll be right 80% of the time!\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **When Model-Centric is Right**\n",
    "\n",
    "Model-centric is the correct priority when:\n",
    "- ✅ Data verified clean (checked all quality metrics)\n",
    "- ✅ Expert domain knowledge confirms data completeness\n",
    "- ✅ Model is obviously too simple (linear for non-linear problem)\n",
    "- ✅ Already tried data improvements with limited success\n",
    "- ✅ Have strong theoretical reason for specific architecture\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **The Synergy**\n",
    "\n",
    "**Best practice**: Combine both approaches\n",
    "1. Start with data-centric (get quick wins)\n",
    "2. Then model-centric (squeeze out remaining performance)\n",
    "3. Iterate between both\n",
    "\n",
    "This gives **compounding improvements**: Good data × Good model = Best results\n",
    "\n",
    "---\n",
    "\n",
    "### Final Thought:\n",
    "\n",
    "> **\"In machine learning, data is the foundation. You can build a skyscraper on solid ground with basic tools, but you can't build anything lasting on quicksand, no matter how advanced your equipment.\"**\n",
    "\n",
    "**Answer to the original question**:\n",
    "\n",
    "**Run DATA-CENTRIC experiments first**, because:\n",
    "1. Data issues are more common\n",
    "2. Faster and cheaper to fix\n",
    "3. Benefits are universal across models\n",
    "4. Fundamental constraint that models can't overcome\n",
    "\n",
    "Only after verifying data quality should you invest in model-centric improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ced6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
