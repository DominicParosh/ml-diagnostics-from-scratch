{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5bde29a",
   "metadata": {},
   "source": [
    "# Bias and Variance: Decision Boundary Stability Across Datasets\n",
    "\n",
    "## Objective\n",
    "Understand bias and variance through the lens of **decision boundary stability**:\n",
    "\n",
    "- **High Bias, Low Variance**: Boundaries are **stable** (consistent across datasets) but **wrong** (don't capture true pattern)\n",
    "- **Low Bias, High Variance**: Boundaries are **unstable** (change dramatically across datasets) but can be accurate on average\n",
    "- **Optimal**: Boundaries are **relatively stable AND accurate**\n",
    "\n",
    "## Key Insight\n",
    "Instead of thinking about bias/variance as abstract statistical properties, we'll visualize them as:\n",
    "- **Variance** = How much the decision boundary **wobbles** when you change the training data\n",
    "- **Bias** = How far the **average** decision boundary is from the true boundary\n",
    "\n",
    "## Approach\n",
    "1. Generate multiple training datasets from the same distribution\n",
    "2. Train models of varying complexity on each dataset\n",
    "3. Visualize how decision boundaries change\n",
    "4. Quantify boundary stability and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f26a53",
   "metadata": {},
   "source": [
    "## Part 1: Synthetic Dataset with Known True Boundary\n",
    "\n",
    "We'll create a 2D binary classification problem where:\n",
    "- **True boundary**: A known, relatively simple curve (e.g., circular or sinusoidal)\n",
    "- **Observed data**: Points sampled from two classes with some noise\n",
    "- **Multiple datasets**: We'll generate many different training sets from the same distribution\n",
    "\n",
    "This allows us to:\n",
    "1. Know the ground truth decision boundary\n",
    "2. See how learned boundaries vary across different samples\n",
    "3. Measure both bias (distance from truth) and variance (instability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f1e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(n_samples=200, noise=0.15, random_state=None):\n",
    "    \"\"\"\n",
    "    Generate 2D classification data with a non-linear true boundary.\n",
    "    True boundary: A circular pattern with some irregularity\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Generate points uniformly in 2D space\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    \n",
    "    # True decision boundary: circular with sinusoidal perturbation\n",
    "    # Formula: r_boundary = 1.5 + 0.3*sin(4*theta)\n",
    "    distances = np.sqrt(X[:, 0]**2 + X[:, 1]**2)\n",
    "    angles = np.arctan2(X[:, 1], X[:, 0])\n",
    "    true_boundary_radius = 1.5 + 0.3 * np.sin(4 * angles)\n",
    "    \n",
    "    # Assign labels based on distance from center (with noise)\n",
    "    y_true = (distances < true_boundary_radius).astype(int)\n",
    "    \n",
    "    # Add noise: flip some labels\n",
    "    n_flip = int(noise * n_samples)\n",
    "    flip_indices = np.random.choice(n_samples, n_flip, replace=False)\n",
    "    y_noisy = y_true.copy()\n",
    "    y_noisy[flip_indices] = 1 - y_noisy[flip_indices]\n",
    "    \n",
    "    return X, y_noisy, y_true\n",
    "\n",
    "# Generate a single dataset for visualization\n",
    "X_example, y_example, y_true_example = generate_classification_data(\n",
    "    n_samples=300, noise=0.15, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(X_example)} samples\")\n",
    "print(f\"Class 0: {np.sum(y_example == 0)} samples\")\n",
    "print(f\"Class 1: {np.sum(y_example == 1)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_boundary(ax, resolution=0.02):\n",
    "    \"\"\"Plot the true decision boundary\"\"\"\n",
    "    # Create a mesh\n",
    "    x_min, x_max = -3, 3\n",
    "    y_min, y_max = -3, 3\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                         np.arange(y_min, y_max, resolution))\n",
    "    \n",
    "    # Calculate true boundary for each point\n",
    "    distances = np.sqrt(xx**2 + yy**2)\n",
    "    angles = np.arctan2(yy, xx)\n",
    "    true_boundary_radius = 1.5 + 0.3 * np.sin(4 * angles)\n",
    "    \n",
    "    # Points inside boundary = class 1, outside = class 0\n",
    "    Z_true = (distances < true_boundary_radius).astype(int)\n",
    "    \n",
    "    # Plot boundary\n",
    "    ax.contour(xx, yy, Z_true, levels=[0.5], colors='green', \n",
    "               linewidths=3, linestyles='--', alpha=0.8)\n",
    "    \n",
    "    return xx, yy, Z_true\n",
    "\n",
    "# Visualize the data and true boundary\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot data points\n",
    "scatter = ax.scatter(X_example[:, 0], X_example[:, 1], \n",
    "                     c=y_example, cmap='coolwarm', \n",
    "                     s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Plot true boundary\n",
    "plot_true_boundary(ax)\n",
    "\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12)\n",
    "ax.set_title('Synthetic Dataset with True Decision Boundary\\n(Green dashed line = true boundary)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor='blue', alpha=0.6, label='Class 0'),\n",
    "    Patch(facecolor='red', alpha=0.6, label='Class 1'),\n",
    "    plt.Line2D([0], [0], color='green', linewidth=3, linestyle='--', label='True Boundary')\n",
    "]\n",
    "ax.legend(handles=legend_elements, fontsize=11, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16875be8",
   "metadata": {},
   "source": [
    "## Part 2: Generate Multiple Training Datasets\n",
    "\n",
    "To understand variance, we need to see how models behave across **different training sets** from the same distribution.\n",
    "\n",
    "We'll create 20 different training datasets by:\n",
    "1. Sampling different subsets from the population\n",
    "2. Each dataset has the same size but different samples\n",
    "\n",
    "This simulates what happens in practice: if you collected data again, you'd get different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple training datasets\n",
    "n_datasets = 20\n",
    "n_train_samples = 150\n",
    "datasets = []\n",
    "\n",
    "for i in range(n_datasets):\n",
    "    X_train, y_train, _ = generate_classification_data(\n",
    "        n_samples=n_train_samples, \n",
    "        noise=0.15, \n",
    "        random_state=i * 10\n",
    "    )\n",
    "    datasets.append((X_train, y_train))\n",
    "\n",
    "print(f\"Generated {n_datasets} different training datasets\")\n",
    "print(f\"Each dataset has {n_train_samples} samples\")\n",
    "\n",
    "# Visualize first 4 datasets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(4):\n",
    "    X_train, y_train = datasets[idx]\n",
    "    \n",
    "    axes[idx].scatter(X_train[:, 0], X_train[:, 1], \n",
    "                     c=y_train, cmap='coolwarm',\n",
    "                     s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    plot_true_boundary(axes[idx])\n",
    "    \n",
    "    axes[idx].set_xlim(-3, 3)\n",
    "    axes[idx].set_ylim(-3, 3)\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=11)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=11)\n",
    "    axes[idx].set_title(f'Training Dataset {idx + 1}', fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Different Training Datasets from Same Distribution', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: Each dataset samples different points, but all follow the same pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6f788",
   "metadata": {},
   "source": [
    "## Part 3: Train Models with Different Complexity\n",
    "\n",
    "We'll train three types of models representing different bias-variance tradeoffs:\n",
    "\n",
    "### 1. **High Bias, Low Variance** (Simple Model)\n",
    "- **Model**: Logistic Regression (linear boundary)\n",
    "- **Expected behavior**: Boundary stays similar across datasets (stable) but can't capture true pattern (biased)\n",
    "\n",
    "### 2. **Balanced** (Moderate Complexity)\n",
    "- **Model**: Random Forest with limited depth\n",
    "- **Expected behavior**: Reasonable stability and reasonable accuracy\n",
    "\n",
    "### 3. **Low Bias, High Variance** (Complex Model)\n",
    "- **Model**: Deep Decision Tree (unlimited depth)\n",
    "- **Expected behavior**: Boundary changes wildly across datasets (unstable) but can fit complex patterns\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df429dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_on_datasets(datasets):\n",
    "    \"\"\"Train three different models on all datasets\"\"\"\n",
    "    \n",
    "    models_config = {\n",
    "        'Linear (High Bias)': LogisticRegression(max_iter=1000),\n",
    "        'Random Forest (Balanced)': RandomForestClassifier(\n",
    "            n_estimators=50, max_depth=5, random_state=42\n",
    "        ),\n",
    "        'Deep Tree (High Variance)': DecisionTreeClassifier(\n",
    "            max_depth=None, min_samples_split=2\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    trained_models = {name: [] for name in models_config.keys()}\n",
    "    \n",
    "    for X_train, y_train in datasets:\n",
    "        for model_name, model_template in models_config.items():\n",
    "            # Clone the model\n",
    "            if model_name == 'Linear (High Bias)':\n",
    "                model = LogisticRegression(max_iter=1000)\n",
    "            elif model_name == 'Random Forest (Balanced)':\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=50, max_depth=5, random_state=42\n",
    "                )\n",
    "            else:\n",
    "                model = DecisionTreeClassifier(\n",
    "                    max_depth=None, min_samples_split=2\n",
    "                )\n",
    "            \n",
    "            # Train\n",
    "            model.fit(X_train, y_train)\n",
    "            trained_models[model_name].append(model)\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "# Train all models\n",
    "print(\"Training models on all datasets...\")\n",
    "trained_models = train_models_on_datasets(datasets)\n",
    "print(f\"✓ Trained {len(trained_models)} model types on {n_datasets} datasets each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00256cb6",
   "metadata": {},
   "source": [
    "## Part 4: Visualize Decision Boundary Stability\n",
    "\n",
    "For each model type, we'll:\n",
    "1. Plot all decision boundaries learned from different datasets (overlaid)\n",
    "2. Calculate the **variance** in predictions at each point in space\n",
    "3. Visualize regions where the boundary is **stable vs unstable**\n",
    "\n",
    "**Key Visualization**:\n",
    "- Each line = decision boundary from one dataset\n",
    "- More spread between lines = higher variance\n",
    "- Consistent lines = low variance (stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries_overlay(models, datasets, model_name, ax, \n",
    "                                     resolution=0.02, show_data=True):\n",
    "    \"\"\"Plot decision boundaries from all trained models overlaid\"\"\"\n",
    "    \n",
    "    # Create mesh\n",
    "    x_min, x_max = -3, 3\n",
    "    y_min, y_max = -3, 3\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                         np.arange(y_min, y_max, resolution))\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # Plot boundaries from each model\n",
    "    for idx, model in enumerate(models):\n",
    "        Z = model.predict(mesh_points).reshape(xx.shape)\n",
    "        \n",
    "        # Plot boundary (contour at decision boundary)\n",
    "        ax.contour(xx, yy, Z, levels=[0.5], colors='blue', \n",
    "                   linewidths=1.5, alpha=0.3)\n",
    "    \n",
    "    # Plot true boundary\n",
    "    plot_true_boundary(ax, resolution=resolution)\n",
    "    \n",
    "    # Optionally plot one dataset for reference\n",
    "    if show_data:\n",
    "        X_train, y_train = datasets[0]\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], \n",
    "                  c=y_train, cmap='coolwarm',\n",
    "                  s=30, alpha=0.4, edgecolors='black', linewidth=0.3)\n",
    "    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xlabel('Feature 1', fontsize=11)\n",
    "    ax.set_ylabel('Feature 2', fontsize=11)\n",
    "    ax.set_title(f'{model_name}\\n({len(models)} boundaries overlaid)', fontsize=12)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "model_names = list(trained_models.keys())\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    plot_decision_boundaries_overlay(\n",
    "        trained_models[model_name], \n",
    "        datasets, \n",
    "        model_name, \n",
    "        axes[idx],\n",
    "        show_data=True\n",
    "    )\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], color='blue', linewidth=1.5, alpha=0.5, label='Learned Boundaries'),\n",
    "    plt.Line2D([0], [0], color='green', linewidth=3, linestyle='--', label='True Boundary'),\n",
    "]\n",
    "axes[0].legend(handles=legend_elements, fontsize=10, loc='upper right')\n",
    "\n",
    "plt.suptitle('Decision Boundary Stability Across Different Training Datasets', \n",
    "             fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"- Linear model: Boundaries are very similar (low variance) but wrong (high bias)\")\n",
    "print(\"- Deep Tree: Boundaries vary wildly (high variance) with different data\")\n",
    "print(\"- Random Forest: Moderate stability with reasonable fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842d6f7",
   "metadata": {},
   "source": [
    "## Part 5: Quantify Decision Boundary Variance\n",
    "\n",
    "Let's measure variance numerically:\n",
    "\n",
    "For each point in the 2D space:\n",
    "1. Get predictions from all 20 models\n",
    "2. Calculate variance of predictions\n",
    "3. High variance = boundary location is unstable at that point\n",
    "\n",
    "**Variance Formula**:\n",
    "For each point (x, y), we have predictions p₁, p₂, ..., p₂₀ from 20 models.\n",
    "\n",
    "Variance = (1/20) Σ (pᵢ - p̄)²\n",
    "\n",
    "Where p̄ is the average prediction across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prediction_variance(models, resolution=0.05):\n",
    "    \"\"\"Calculate variance in predictions across models\"\"\"\n",
    "    \n",
    "    # Create mesh\n",
    "    x_min, x_max = -3, 3\n",
    "    y_min, y_max = -3, 3\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                         np.arange(y_min, y_max, resolution))\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # Collect predictions from all models\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            # Get probability of class 1\n",
    "            pred = model.predict_proba(mesh_points)[:, 1]\n",
    "        else:\n",
    "            pred = model.predict(mesh_points)\n",
    "        all_predictions.append(pred)\n",
    "    \n",
    "    # Stack predictions: shape (n_models, n_points)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    \n",
    "    # Calculate variance across models for each point\n",
    "    prediction_variance = np.var(all_predictions, axis=0)\n",
    "    \n",
    "    # Calculate mean prediction\n",
    "    prediction_mean = np.mean(all_predictions, axis=0)\n",
    "    \n",
    "    return xx, yy, prediction_variance.reshape(xx.shape), prediction_mean.reshape(xx.shape)\n",
    "\n",
    "# Calculate variance for each model type\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    xx, yy, variance, mean_pred = calculate_prediction_variance(\n",
    "        trained_models[model_name], resolution=0.05\n",
    "    )\n",
    "    \n",
    "    # Plot variance as heatmap\n",
    "    im = axes[idx].contourf(xx, yy, variance, levels=20, cmap='YlOrRd', alpha=0.8)\n",
    "    \n",
    "    # Overlay true boundary\n",
    "    plot_true_boundary(axes[idx], resolution=0.02)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=axes[idx])\n",
    "    cbar.set_label('Prediction Variance', fontsize=10)\n",
    "    \n",
    "    axes[idx].set_xlim(-3, 3)\n",
    "    axes[idx].set_ylim(-3, 3)\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=11)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=11)\n",
    "    axes[idx].set_title(f'{model_name}\\nPrediction Variance Map', fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.2)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Mean variance: {np.mean(variance):.4f}\")\n",
    "    print(f\"  Max variance: {np.max(variance):.4f}\")\n",
    "    print(f\"  Std variance: {np.std(variance):.4f}\\n\")\n",
    "\n",
    "plt.suptitle('Decision Boundary Variance Across Datasets\\n(Red = high variance/instability, Yellow = low variance/stability)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0541864",
   "metadata": {},
   "source": [
    "## Part 6: Quantify Bias\n",
    "\n",
    "**Bias** = How far is the **average** learned boundary from the **true** boundary?\n",
    "\n",
    "For each model type:\n",
    "1. Average predictions from all 20 models at each point\n",
    "2. Compare to true labels\n",
    "3. Calculate the difference\n",
    "\n",
    "**Bias Formula**:\n",
    "Bias² = (E[f̂(x)] - f_true(x))²\n",
    "\n",
    "Where:\n",
    "- E[f̂(x)] = average prediction across all trained models\n",
    "- f_true(x) = true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bias(models, resolution=0.05):\n",
    "    \"\"\"Calculate squared bias compared to true boundary\"\"\"\n",
    "    \n",
    "    # Create mesh\n",
    "    x_min, x_max = -3, 3\n",
    "    y_min, y_max = -3, 3\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                         np.arange(y_min, y_max, resolution))\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # True labels\n",
    "    distances = np.sqrt(mesh_points[:, 0]**2 + mesh_points[:, 1]**2)\n",
    "    angles = np.arctan2(mesh_points[:, 1], mesh_points[:, 0])\n",
    "    true_boundary_radius = 1.5 + 0.3 * np.sin(4 * angles)\n",
    "    y_true = (distances < true_boundary_radius).astype(float)\n",
    "    \n",
    "    # Collect predictions from all models\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            pred = model.predict_proba(mesh_points)[:, 1]\n",
    "        else:\n",
    "            pred = model.predict(mesh_points).astype(float)\n",
    "        all_predictions.append(pred)\n",
    "    \n",
    "    # Average prediction across models\n",
    "    mean_prediction = np.mean(all_predictions, axis=0)\n",
    "    \n",
    "    # Squared bias\n",
    "    squared_bias = (mean_prediction - y_true) ** 2\n",
    "    \n",
    "    return xx, yy, squared_bias.reshape(xx.shape), mean_prediction.reshape(xx.shape)\n",
    "\n",
    "# Calculate bias for each model type\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    xx, yy, squared_bias, mean_pred = calculate_bias(\n",
    "        trained_models[model_name], resolution=0.05\n",
    "    )\n",
    "    \n",
    "    # Plot squared bias as heatmap\n",
    "    im = axes[idx].contourf(xx, yy, squared_bias, levels=20, cmap='Blues', alpha=0.8)\n",
    "    \n",
    "    # Overlay true boundary\n",
    "    plot_true_boundary(axes[idx], resolution=0.02)\n",
    "    \n",
    "    # Overlay average learned boundary\n",
    "    axes[idx].contour(xx, yy, mean_pred, levels=[0.5], colors='red', \n",
    "                      linewidths=3, linestyles='-', alpha=0.8)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=axes[idx])\n",
    "    cbar.set_label('Squared Bias', fontsize=10)\n",
    "    \n",
    "    axes[idx].set_xlim(-3, 3)\n",
    "    axes[idx].set_ylim(-3, 3)\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=11)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=11)\n",
    "    axes[idx].set_title(f'{model_name}\\nSquared Bias Map', fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.2)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Mean squared bias: {np.mean(squared_bias):.4f}\")\n",
    "    print(f\"  Max squared bias: {np.max(squared_bias):.4f}\\n\")\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], color='green', linewidth=3, linestyle='--', label='True Boundary'),\n",
    "    plt.Line2D([0], [0], color='red', linewidth=3, linestyle='-', label='Average Learned Boundary'),\n",
    "]\n",
    "axes[0].legend(handles=legend_elements, fontsize=10, loc='upper right')\n",
    "\n",
    "plt.suptitle('Bias: Distance from True Boundary\\n(Darker blue = higher bias)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09c4f7",
   "metadata": {},
   "source": [
    "## Part 7: Complete Bias-Variance Decomposition\n",
    "\n",
    "The expected prediction error can be decomposed as:\n",
    "\n",
    "**Expected Error = Bias² + Variance + Irreducible Error**\n",
    "\n",
    "Let's visualize all three components for each model type:\n",
    "\n",
    "1. **Bias²**: How far average prediction is from truth\n",
    "2. **Variance**: How much predictions fluctuate across datasets  \n",
    "3. **Total Error**: Bias² + Variance (ignoring irreducible error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f618233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias_variance_stats(models, resolution=0.08):\n",
    "    \"\"\"Compute average bias and variance statistics\"\"\"\n",
    "    \n",
    "    # Create mesh\n",
    "    x_min, x_max = -3, 3\n",
    "    y_min, y_max = -3, 3\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                         np.arange(y_min, y_max, resolution))\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # True labels\n",
    "    distances = np.sqrt(mesh_points[:, 0]**2 + mesh_points[:, 1]**2)\n",
    "    angles = np.arctan2(mesh_points[:, 1], mesh_points[:, 0])\n",
    "    true_boundary_radius = 1.5 + 0.3 * np.sin(4 * angles)\n",
    "    y_true = (distances < true_boundary_radius).astype(float)\n",
    "    \n",
    "    # Collect predictions\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            pred = model.predict_proba(mesh_points)[:, 1]\n",
    "        else:\n",
    "            pred = model.predict(mesh_points).astype(float)\n",
    "        all_predictions.append(pred)\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mean_prediction = np.mean(all_predictions, axis=0)\n",
    "    variance = np.var(all_predictions, axis=0)\n",
    "    squared_bias = (mean_prediction - y_true) ** 2\n",
    "    \n",
    "    # Average over all points\n",
    "    avg_bias_sq = np.mean(squared_bias)\n",
    "    avg_variance = np.mean(variance)\n",
    "    total_error = avg_bias_sq + avg_variance\n",
    "    \n",
    "    return avg_bias_sq, avg_variance, total_error\n",
    "\n",
    "# Compute for all model types\n",
    "results = {}\n",
    "for model_name in model_names:\n",
    "    bias_sq, variance, total = compute_bias_variance_stats(\n",
    "        trained_models[model_name]\n",
    "    )\n",
    "    results[model_name] = {\n",
    "        'bias_squared': bias_sq,\n",
    "        'variance': variance,\n",
    "        'total_error': total\n",
    "    }\n",
    "\n",
    "# Create summary visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart\n",
    "model_labels = list(results.keys())\n",
    "bias_values = [results[m]['bias_squared'] for m in model_labels]\n",
    "variance_values = [results[m]['variance'] for m in model_labels]\n",
    "total_values = [results[m]['total_error'] for m in model_labels]\n",
    "\n",
    "x_pos = np.arange(len(model_labels))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = axes[0].bar(x_pos - width, bias_values, width, \n",
    "                    label='Bias²', color='steelblue', alpha=0.8)\n",
    "bars2 = axes[0].bar(x_pos, variance_values, width, \n",
    "                    label='Variance', color='coral', alpha=0.8)\n",
    "bars3 = axes[0].bar(x_pos + width, total_values, width, \n",
    "                    label='Total (Bias² + Variance)', color='purple', alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Model Type', fontsize=12)\n",
    "axes[0].set_ylabel('Error Magnitude', fontsize=12)\n",
    "axes[0].set_title('Bias-Variance Decomposition by Model Type', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(model_labels, fontsize=10)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.4f}',\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Stacked bar chart\n",
    "bias_sq_vals = [results[m]['bias_squared'] for m in model_labels]\n",
    "var_vals = [results[m]['variance'] for m in model_labels]\n",
    "\n",
    "axes[1].bar(model_labels, bias_sq_vals, label='Bias²', \n",
    "            color='steelblue', alpha=0.8)\n",
    "axes[1].bar(model_labels, var_vals, bottom=bias_sq_vals, \n",
    "            label='Variance', color='coral', alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Model Type', fontsize=12)\n",
    "axes[1].set_ylabel('Error Magnitude', fontsize=12)\n",
    "axes[1].set_title('Stacked View: Bias² + Variance = Total Error', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].tick_params(axis='x', labelsize=10)\n",
    "\n",
    "# Add total labels\n",
    "for i, model in enumerate(model_labels):\n",
    "    total = results[model]['total_error']\n",
    "    axes[1].text(i, total + 0.002, f'{total:.4f}',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BIAS-VARIANCE DECOMPOSITION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for model_name in model_labels:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Bias²:       {results[model_name]['bias_squared']:.6f}\")\n",
    "    print(f\"  Variance:    {results[model_name]['variance']:.6f}\")\n",
    "    print(f\"  Total Error: {results[model_name]['total_error']:.6f}\")\n",
    "    print(f\"  Ratio (Var/Bias²): {results[model_name]['variance']/results[model_name]['bias_squared']:.2f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d3fe9",
   "metadata": {},
   "source": [
    "## Part 8: Quantifying Boundary Stability\n",
    "\n",
    "Let's create a specific metric for **decision boundary stability**:\n",
    "\n",
    "For each pair of models trained on different datasets:\n",
    "1. Calculate the **average distance** between their decision boundaries\n",
    "2. Lower distance = more stable boundaries\n",
    "\n",
    "We'll measure this by sampling points along each boundary and computing distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f2797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_boundary_stability(models, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Measure how much decision boundaries vary across models.\n",
    "    Returns average disagreement rate between model pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample random points in the space\n",
    "    np.random.seed(42)\n",
    "    X_sample = np.random.uniform(-3, 3, (n_samples, 2))\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model.predict(X_sample)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)  # shape: (n_models, n_samples)\n",
    "    \n",
    "    # Calculate pairwise disagreement\n",
    "    n_models = len(models)\n",
    "    disagreements = []\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        for j in range(i + 1, n_models):\n",
    "            disagreement_rate = np.mean(predictions[i] != predictions[j])\n",
    "            disagreements.append(disagreement_rate)\n",
    "    \n",
    "    avg_disagreement = np.mean(disagreements)\n",
    "    std_disagreement = np.std(disagreements)\n",
    "    \n",
    "    return avg_disagreement, std_disagreement, disagreements\n",
    "\n",
    "# Calculate stability for each model type\n",
    "print(\"DECISION BOUNDARY STABILITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"(Disagreement rate = % of points where two models predict differently)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "stability_results = {}\n",
    "for model_name in model_names:\n",
    "    avg_dis, std_dis, all_dis = calculate_boundary_stability(\n",
    "        trained_models[model_name]\n",
    "    )\n",
    "    stability_results[model_name] = {\n",
    "        'mean': avg_dis,\n",
    "        'std': std_dis,\n",
    "        'all': all_dis\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Average disagreement rate: {avg_dis:.4f} ({avg_dis*100:.2f}%)\")\n",
    "    print(f\"  Std of disagreement: {std_dis:.4f}\")\n",
    "    print(f\"  Interpretation: {'STABLE' if avg_dis < 0.1 else 'MODERATE' if avg_dis < 0.25 else 'UNSTABLE'}\")\n",
    "    print()\n",
    "\n",
    "# Visualize as box plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "data_to_plot = [stability_results[m]['all'] for m in model_names]\n",
    "bp = ax.boxplot(data_to_plot, labels=model_names, patch_artist=True,\n",
    "                showmeans=True, meanline=True)\n",
    "\n",
    "# Color boxes\n",
    "colors = ['steelblue', 'lightgreen', 'coral']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('Pairwise Disagreement Rate', fontsize=12)\n",
    "ax.set_xlabel('Model Type', fontsize=12)\n",
    "ax.set_title('Decision Boundary Stability Across Different Datasets\\n(Lower = more stable/consistent boundaries)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.tick_params(axis='x', labelsize=10)\n",
    "\n",
    "# Add horizontal reference lines\n",
    "ax.axhline(0.1, color='green', linestyle='--', alpha=0.3, label='High Stability')\n",
    "ax.axhline(0.25, color='orange', linestyle='--', alpha=0.3, label='Moderate Stability')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e5295",
   "metadata": {},
   "source": [
    "## Part 9: Side-by-Side Comparison\n",
    "\n",
    "Let's create a comprehensive comparison showing:\n",
    "1. Individual boundaries from different datasets\n",
    "2. Variance heatmap\n",
    "3. Average boundary vs true boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8704098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive 3x3 grid\n",
    "fig = plt.figure(figsize=(20, 18))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "resolution = 0.04\n",
    "\n",
    "for row, model_name in enumerate(model_names):\n",
    "    models = trained_models[model_name]\n",
    "    \n",
    "    # Column 0: Overlay of all boundaries\n",
    "    ax0 = fig.add_subplot(gs[row, 0])\n",
    "    plot_decision_boundaries_overlay(models, datasets, model_name, ax0, \n",
    "                                     resolution=resolution, show_data=True)\n",
    "    \n",
    "    # Column 1: Variance heatmap\n",
    "    ax1 = fig.add_subplot(gs[row, 1])\n",
    "    xx, yy, variance, mean_pred = calculate_prediction_variance(models, resolution=resolution)\n",
    "    im1 = ax1.contourf(xx, yy, variance, levels=20, cmap='YlOrRd', alpha=0.8)\n",
    "    plot_true_boundary(ax1, resolution=0.02)\n",
    "    ax1.set_xlim(-3, 3)\n",
    "    ax1.set_ylim(-3, 3)\n",
    "    ax1.set_xlabel('Feature 1', fontsize=11)\n",
    "    ax1.set_ylabel('Feature 2', fontsize=11)\n",
    "    ax1.set_title(f'Variance Map\\nMean: {np.mean(variance):.4f}', fontsize=11)\n",
    "    ax1.grid(True, alpha=0.2)\n",
    "    plt.colorbar(im1, ax=ax1, label='Variance')\n",
    "    \n",
    "    # Column 2: Bias heatmap\n",
    "    ax2 = fig.add_subplot(gs[row, 2])\n",
    "    xx, yy, squared_bias, mean_pred = calculate_bias(models, resolution=resolution)\n",
    "    im2 = ax2.contourf(xx, yy, squared_bias, levels=20, cmap='Blues', alpha=0.8)\n",
    "    plot_true_boundary(ax2, resolution=0.02)\n",
    "    ax2.contour(xx, yy, mean_pred, levels=[0.5], colors='red', \n",
    "                linewidths=3, alpha=0.8)\n",
    "    ax2.set_xlim(-3, 3)\n",
    "    ax2.set_ylim(-3, 3)\n",
    "    ax2.set_xlabel('Feature 1', fontsize=11)\n",
    "    ax2.set_ylabel('Feature 2', fontsize=11)\n",
    "    ax2.set_title(f'Bias² Map\\nMean: {np.mean(squared_bias):.4f}', fontsize=11)\n",
    "    ax2.grid(True, alpha=0.2)\n",
    "    plt.colorbar(im2, ax=ax2, label='Bias²')\n",
    "\n",
    "# Add overall title\n",
    "plt.suptitle('Complete Bias-Variance Analysis: Boundary Stability Across Datasets', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc45e82",
   "metadata": {},
   "source": [
    "## Summary: Bias and Variance Through Decision Boundary Stability\n",
    "\n",
    "### What We've Demonstrated:\n",
    "\n",
    "### 1. **High Bias, Low Variance (Linear Model)**\n",
    "   - ✅ **Stable**: Boundaries barely change across different datasets\n",
    "   - ❌ **Biased**: Cannot capture the true curved boundary\n",
    "   - **Geometric**: The hypothesis space (linear boundaries) doesn't contain the true boundary\n",
    "   - **Result**: Consistently wrong predictions, but at least consistent!\n",
    "\n",
    "### 2. **Low Bias, High Variance (Deep Tree)**\n",
    "   - ✅ **Flexible**: Can approximate the true boundary closely\n",
    "   - ❌ **Unstable**: Wild variations in boundaries across datasets\n",
    "   - **Geometric**: Different training sets lead to drastically different decision boundaries\n",
    "   - **Result**: Sometimes very accurate, sometimes very wrong - unpredictable!\n",
    "\n",
    "### 3. **Balanced (Random Forest)**\n",
    "   - ✅ **Moderately Stable**: Boundaries are reasonably consistent\n",
    "   - ✅ **Reasonably Accurate**: Captures the general pattern\n",
    "   - **Geometric**: Small perturbations in data cause small changes in boundary\n",
    "   - **Result**: Best overall performance through bias-variance tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights:\n",
    "\n",
    "### Variance as Boundary Instability:\n",
    "- **High variance** means: \"If I collect new training data, my decision boundary will be completely different\"\n",
    "- Geometrically: The learned function f̂ is extremely sensitive to which specific samples you observe\n",
    "- Visually: The cloud of boundaries spreads widely across space\n",
    "\n",
    "### Bias as Systematic Error:\n",
    "- **High bias** means: \"Even averaging over all possible training sets, my boundary is still wrong\"\n",
    "- Geometrically: The average learned boundary E[f̂] is far from the true boundary f*\n",
    "- Visually: The center of the boundary cloud is offset from the true boundary\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Connection:\n",
    "```\n",
    "At each point x in feature space:\n",
    "\n",
    "Expected Error(x) = Bias²(x) + Variance(x) + Noise\n",
    "\n",
    "Where:\n",
    "- Bias²(x) = (E[f̂(x)] - f*(x))²     [Average boundary vs truth]\n",
    "- Variance(x) = E[(f̂(x) - E[f̂(x)])²]  [Spread of boundaries]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implications:\n",
    "\n",
    "1. **Model Selection**: Choose complexity based on:\n",
    "   - How much data you have (more data → can handle more complexity)\n",
    "   - How stable you need predictions to be (production systems → favor stability)\n",
    "   - Whether you can ensemble (averaging reduces variance)\n",
    "\n",
    "2. **Diagnosing Issues**:\n",
    "   - Stable but wrong boundaries → **increase model complexity**\n",
    "   - Unstable boundaries → **reduce complexity or add regularization**\n",
    "   - Both unstable AND wrong → **need more/better data**\n",
    "\n",
    "3. **Boundary Stability as a Metric**:\n",
    "   - Train multiple models on bootstrap samples\n",
    "   - Measure disagreement rate\n",
    "   - High disagreement = warning sign for high variance\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "**Bias and variance are not just abstract statistical concepts** - they have clear geometric interpretations:\n",
    "\n",
    "- **Variance** = How much your decision boundary **wobbles** with different data\n",
    "- **Bias** = How far your **average** boundary is from the **truth**\n",
    "\n",
    "The art of machine learning is finding models whose boundaries are both **stable** (low variance) and **accurate** (low bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*20 + \"FINAL SUMMARY: BIAS-VARIANCE-STABILITY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model Type':<30} {'Bias²':<12} {'Variance':<12} {'Total':<12} {'Stability':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for model_name in model_names:\n",
    "    bias_sq = results[model_name]['bias_squared']\n",
    "    variance = results[model_name]['variance']\n",
    "    total = results[model_name]['total_error']\n",
    "    stability = stability_results[model_name]['mean']\n",
    "    \n",
    "    # Determine category\n",
    "    if \"Linear\" in model_name:\n",
    "        category = \"High Bias\"\n",
    "    elif \"Tree\" in model_name:\n",
    "        category = \"High Variance\"\n",
    "    else:\n",
    "        category = \"Balanced\"\n",
    "    \n",
    "    print(f\"{model_name:<30} {bias_sq:<12.6f} {variance:<12.6f} {total:<12.6f} {stability:<12.4f}\")\n",
    "    print(f\"  └─ {category}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStability = Average disagreement rate between model pairs (lower is better)\")\n",
    "print(\"Bias² = Squared distance from true boundary (lower is better)\")\n",
    "print(\"Variance = Instability in predictions across datasets (lower is better)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize final tradeoff\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "bias_vals = [results[m]['bias_squared'] for m in model_names]\n",
    "var_vals = [results[m]['variance'] for m in model_names]\n",
    "colors_scatter = ['steelblue', 'lightgreen', 'coral']\n",
    "sizes = [300, 300, 300]\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    ax.scatter(bias_vals[i], var_vals[i], s=sizes[i], \n",
    "               c=colors_scatter[i], alpha=0.7, edgecolors='black', linewidth=2,\n",
    "               label=model_name)\n",
    "    \n",
    "    # Add annotations\n",
    "    ax.annotate(model_name, \n",
    "                xy=(bias_vals[i], var_vals[i]),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor=colors_scatter[i], alpha=0.3))\n",
    "\n",
    "ax.set_xlabel('Bias² (Systematic Error)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Variance (Instability)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('The Bias-Variance Tradeoff\\n(Optimal is bottom-left corner)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "\n",
    "# Add ideal region\n",
    "ax.axhline(0.01, color='green', linestyle='--', alpha=0.3, linewidth=2)\n",
    "ax.axvline(0.01, color='green', linestyle='--', alpha=0.3, linewidth=2)\n",
    "ax.fill_between([0, 0.01], 0, 0.01, alpha=0.1, color='green', label='Ideal Region')\n",
    "\n",
    "# Add arrows showing tradeoff\n",
    "ax.annotate('', xy=(bias_vals[2], var_vals[2]), xytext=(bias_vals[0], var_vals[0]),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='gray', alpha=0.5))\n",
    "ax.text((bias_vals[0] + bias_vals[2])/2, (var_vals[0] + var_vals[2])/2 + 0.002,\n",
    "        'Complexity\\nIncreases', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5f24b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
